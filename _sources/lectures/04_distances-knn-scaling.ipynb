{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 4: Distances, $k$-nearest neighbours, Imputation, Scaling  \n",
    "\n",
    "UBC 2020-21\n",
    "\n",
    "Instructor: Varada Kolhatkar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lecture learning objectives\n",
    "\n",
    "From this lecture, you will be able to \n",
    "\n",
    "- explain the notion of similarity-based algorithms; \n",
    "- broadly describe how $k$-NNs use distances; \n",
    "- discuss the effect of using a small/large value of the hyperparameter $k$ when using the $k$-NN algorithm; \n",
    "- describe the problem of curse of dimensionality; \n",
    "- explain the general idea of SVMs with RBF kernel;\n",
    "- broadly describe the relation of `gamma` and `C` hyperparameters of SVMs with the fundamental tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from IPython.display import HTML\n",
    "\n",
    "sys.path.append(\"code/.\")\n",
    "\n",
    "import mglearn\n",
    "from plotting_functions import *\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from utils import *\n",
    "from ipywidgets import interactive\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation and big picture <a name=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Analogy-based models\n",
    "\n",
    "- Suppose you are given the following training examples with corresponding labels and are asked to label a given test example.\n",
    "\n",
    "<img src='./img/knn-motivation.png' width=\"1500\">\n",
    "\n",
    "[source](https://vipl.ict.ac.cn/en/database.php)\n",
    "\n",
    "- An intuitive way to classify the test example is by finding the most \"similar\" example(s) from the training set and using that label for the test example.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Analogy-based algorithms in practice\n",
    "\n",
    "- [Herta's High-tech Facial Recognition](https://www.hertasecurity.com/en)\n",
    "    - Feature vectors for human faces \n",
    "    - $k$-NN to identify which face is on their watch list\n",
    "- Recommendation systems     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General idea of $k$-nearest neighbours algorithm\n",
    "\n",
    "- Consider the following toy dataset with two classes.\n",
    "    - blue circles $\\rightarrow$ class 0\n",
    "    - red triangles $\\rightarrow$ class 1 \n",
    "    - green stars $\\rightarrow$ test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kvarada/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/sklearn/utils/deprecation.py:86: FutureWarning: Function make_blobs is deprecated; Please import make_blobs directly from scikit-learn\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X, y = mglearn.datasets.make_forge()\n",
    "X_test = np.array([[8.2, 3.66214339], [9.9, 3.2], [11.2, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_train_test_points(X, y, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given a new data point, predict the class of the data point by finding the \"closest\" data point in the training set, i.e., by finding its \"nearest neighbour\" or majority vote of nearest neighbours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(n_neighbors):\n",
    "    print(n_neighbors)\n",
    "    return plot_knn_clf(X, y, X_test, n_neighbors=n_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0824c93253f842f49735d26b6a268c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_neighbors', max=7, min=1, step=2), Output()), _dom_claâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f(n_neighbors)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact\n",
    "\n",
    "interact(\n",
    "    f, n_neighbors=widgets.IntSlider(min=1, max=7, step=2, value=1),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_knn_clf(X, y, X_test, n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_knn_clf(X, y, X_test, n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distances "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Geometric view of tabular data and dimensions \n",
    "\n",
    "- To understand analogy-based algorithms it's useful to think of data as points in a high dimensional space. \n",
    "- Our `X` represents the the problem in terms of relevant **features** ($d$) with one dimension for each **feature** (column).\n",
    "- Examples are **points in a $d$-dimensional space**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many dimensions (features) are there in the cities data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cities_df = pd.read_csv(\"data/canada_usa_cities.csv\")\n",
    "X_cities = cities_df[[\"longitude\", \"latitude\"]]\n",
    "y_cities = cities_df[\"country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X.iloc[:, 0], X.iloc[:, 1], y)\n",
    "plt.xlabel(\"longitude\")\n",
    "plt.ylabel(\"latitude\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Recall the [Spotify Song Attributes](https://www.kaggle.com/geomack/spotifyclassification/home) dataset from homework 1. \n",
    "- How many dimensions (features) we used in the homework? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "spotify_df = pd.read_csv(\"data/spotify.csv\", index_col=0)\n",
    "X_spotify = spotify_df.drop(columns=[\"target\", \"song_title\", \"artist\"])\n",
    "print(\"The number of features in the Spotify dataset: %d\" % X_spotify.shape[1])\n",
    "X_spotify.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature vectors \n",
    "\n",
    "**Feature vector**\n",
    ": is composed of feature values associated with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"An example feature vector from the cities dataset: %s\" % (X_cities.iloc[0].to_numpy())\n",
    ")\n",
    "print(\n",
    "    \"An example feature vector from the Spotify dataset: \\n%s\" % (X_spotify.iloc[0].to_numpy())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "### Dimensions in ML problems \n",
    "\n",
    "In ML, usually we deal with high dimensional problems where examples are hard to visualize.  \n",
    "\n",
    "- $d \\approx 20$ is considered low dimensional\n",
    "- $d \\approx 1000$ is considered medium dimensional \n",
    "- $d \\approx 100,000$ is considered high dimensional "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature vectors \n",
    "\n",
    "**Feature vector**\n",
    ": is composed of feature values associated with an example.\n",
    "\n",
    "Some example feature vectors are shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "two_cities = train_df.sample(2, random_state=42).drop(columns=[\"country\"])\n",
    "two_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(canada[\"longitude\"], canada[\"latitude\"], color=\"red\", alpha=0.4)\n",
    "plt.scatter(usa[\"longitude\"], usa[\"latitude\"], color=\"blue\", alpha=0.4)\n",
    "plt.scatter(two_cities[\"longitude\"], two_cities[\"latitude\"], color=\"black\", s=300)\n",
    "plt.ylabel(\"latitude\")\n",
    "plt.xlabel(\"longitude\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Similarity between examples\n",
    "\n",
    "Let's take 2 points (two feature vectors) from the cities dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's take 2 points (two feature vectors) from the cities dataset.\n",
    "two_cities = X_cities.sample(2, random_state=1)\n",
    "two_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X_cities.iloc[:, 0], X_cities.iloc[:, 1], y_cities, s=8);\n",
    "mglearn.discrete_scatter(two_cities.iloc[:, 0], two_cities.iloc[:, 1],markers=\"*\", c='g', s=18);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Distance between vectors \n",
    "\n",
    "- For the cities at the two big circles, what is the _distance_ between them?\n",
    "- A common way to calculate the distance between vectors is calculating the **Euclidean distance**. \n",
    "- The euclidean distance between vectors $u = <u_1, u_2, \\dots, u_n>$ and $v = <v_1, v_2, \\dots, v_n>$ is defined as: \n",
    "\n",
    "$$distance(u, v) = \\sqrt{\\sum_{i =1}^{n} (u_i - v_i)^2}$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two sampled points are shown as big black circles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Euclidean distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "two_cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Subtract the two cities\n",
    "- Square the difference\n",
    "- Sum them up \n",
    "- Take the square root "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract the two cities\n",
    "print(\"Subtract the cities: \\n%s\\n\" % (two_cities.iloc[1] - two_cities.iloc[0]))\n",
    "\n",
    "# Squared sum of the difference\n",
    "print(\n",
    "    \"Sum of squares: %0.4f\" % (np.sum((two_cities.iloc[1] - two_cities.iloc[0]) ** 2))\n",
    ")\n",
    "\n",
    "# Take the square root\n",
    "print(\n",
    "    \"Euclidean distance between cities: %0.4f\"\n",
    "    % (np.sqrt(np.sum((two_cities.iloc[1] - two_cities.iloc[0]) ** 2)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "two_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Euclidean distance using sklearn\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "euclidean_distances(two_cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Note: `scikit-learn` supports a number of other [distance metrics](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finding the nearest neighbour\n",
    "\n",
    "- Let's look at distances from all cities to all other cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dists = euclidean_distances(train_df[[\"latitude\", \"longitude\"]])\n",
    "np.fill_diagonal(dists, np.inf)\n",
    "print(\"All distances: %s\\n\\n%s\" % (dists.shape, dists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at the distances between City 0 and some other cities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Feature vector for city 0: \\n%s\\n\" % (train_df.iloc[0]))\n",
    "print(\"Distances from city 0 to the first 5 cities: %s\" % (dists[0][:5]))\n",
    "# We can find the closest city with `np.argmin`:\n",
    "print(\n",
    "    \"The closest city from city 0 is: %d \\n\\nwith feature vector: \\n%s\"\n",
    "    % (np.argmin(dists[0]), train_df.iloc[157])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so the closest city to City 0 is City 157. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Question\n",
    "\n",
    "- Why did we set the diagonal entries to infinity before finding the closest city?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finding the distances to a query point\n",
    "\n",
    "We can also find the distances to a new \"test\" or \"query\" city:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find a city that's closest to the a query city\n",
    "query_point = [[-80, 25]]\n",
    "dists = euclidean_distances(train_df[[\"longitude\", \"latitude\"]], query_point)\n",
    "dists[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The query point is closest to\n",
    "print(\n",
    "    \"The query point %s is closest to the city with index %d and the distance between them is: %0.4f\"\n",
    "    % (query_point, np.argmin(dists), dists[np.argmin(dists)])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $k$-Nearest Neighbours ($k$-NNs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_cities():\n",
    "    plt.scatter(canada[\"longitude\"], canada[\"latitude\"], color=\"red\", alpha=0.6)\n",
    "    plt.scatter(usa[\"longitude\"], usa[\"latitude\"], color=\"blue\", alpha=0.6)\n",
    "    plt.scatter(one_city[\"longitude\"], one_city[\"latitude\"], color=\"black\", s=300)\n",
    "    plt.ylabel(\"latitude\")\n",
    "    plt.xlabel(\"longitude\")\n",
    "    plt.legend(labels=[\"Canada\", \"USA\"], loc=1)\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "small_cities = cities_df.sample(30, random_state=90)\n",
    "one_city = small_cities.sample(1, random_state=44)\n",
    "small_train_df = pd.concat([small_cities, one_city]).drop_duplicates(keep=False)\n",
    "canada = small_cities.query('country == \"Canada\"')\n",
    "usa = small_cities.query('country == \"USA\"')\n",
    "plot_cities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to predict the class of the black point.  \n",
    "- An intuitive way to do this is predict the same label as the \"closest\" point ($k = 1$) (1-nearest neighbour)\n",
    "- We would predict a target of **USA** (blue) in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about using $k > 1$ to get a more robust estimate? \n",
    "- For example, we could also use the 3 closest points (*k* = 3) and let them **vote** on the correct class.  \n",
    "- The **Canada** class (red) would win in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X_train = small_train_df.drop(columns=[\"country\"])\n",
    "y_train = small_train_df[\"country\"]\n",
    "\n",
    "k_values = [1, 3]\n",
    "\n",
    "for k in k_values:\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "    neigh.fit(X_train, y_train)\n",
    "    print(\n",
    "        \"Prediction of the black dot with %d neighbours: %s\"\n",
    "        % (k, neigh.predict(one_city.drop(columns=[\"country\"])))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- Is it a good or a bad idea to consider an odd number for $k$? Why or why not? \n",
    "- Try different values of $k$ in the above code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choosing `n_neighbors`\n",
    "\n",
    "- The primary hyperparameter of the model is `n_neighbors` ($k$) which decides how many neighbours should vote during prediction? \n",
    "- What happens when we play around with `n_neighbors`?\n",
    "- Are we more likely to overfit with a low `n_neighbors` or a high `n_neighbors`?\n",
    "- Let's examine the effect of the hyperparameter on our cities data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cities_df.drop(columns=[\"country\"])\n",
    "y = cities_df[\"country\"]\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "k = 1\n",
    "knn1 = KNeighborsClassifier(n_neighbors=k)\n",
    "scores = cross_validate(knn1, X_train, y_train, return_train_score=True)\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "k = 100\n",
    "knn100 = KNeighborsClassifier(n_neighbors=k)\n",
    "scores = cross_validate(knn100, X_train, y_train, return_train_score=True)\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "knn1.fit(X_train, y_train)\n",
    "plt.title(\"n_neighbors = 1\")\n",
    "plot_classifier(X_train, y_train, knn1, ax=plt.gca(), ticks=True)\n",
    "plt.ylabel(\"latitude\")\n",
    "plt.xlabel(\"longitude\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"n_neighbors = 100\")\n",
    "knn100.fit(X_train, y_train)\n",
    "plot_classifier(X_train, y_train, knn100, ax=plt.gca(), ticks=True)\n",
    "plt.ylabel(\"latitude\")\n",
    "plt.xlabel(\"longitude\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to choose `n_neighbors`?\n",
    "\n",
    "- `n_neighbors` is a hyperparameter\n",
    "- We can use hyperparameter optimization to choose `n_neighbors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {\n",
    "    \"n_neighbors\": [],\n",
    "    \"mean_train_score\": [],\n",
    "    \"mean_cv_score\": [],\n",
    "    \"std_cv_score\": [],\n",
    "    \"std_train_score\": [],\n",
    "}\n",
    "param_grid = {\"n_neighbors\": np.arange(1, 50, 5)}\n",
    "\n",
    "for k in param_grid[\"n_neighbors\"]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_validate(knn, X_train, y_train, return_train_score=True)\n",
    "    results_dict[\"n_neighbors\"].append(k)\n",
    "\n",
    "    results_dict[\"mean_cv_score\"].append(np.mean(scores[\"test_score\"]))\n",
    "    results_dict[\"mean_train_score\"].append(np.mean(scores[\"train_score\"]))\n",
    "    results_dict[\"std_cv_score\"].append(scores[\"test_score\"].std())\n",
    "    results_dict[\"std_train_score\"].append(scores[\"train_score\"].std())\n",
    "\n",
    "results_df = pd.DataFrame(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "results_df = results_df.set_index(\"n_neighbors\")\n",
    "results_df[[\"mean_train_score\", \"mean_cv_score\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results_df = results_df.sort_values(\"mean_cv_score\", ascending=False)\n",
    "sorted_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=11)\n",
    "knn.fit(X_train, y_train)\n",
    "print(\"Test accuracy: %0.3f\" % (knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question for you\n",
    "\n",
    "- Why do we have to treat $k$ as a hyperparameter rather than a parameter? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other useful arguments of `KNeighborsClassifier`\n",
    "\n",
    "- `weights` $\\rightarrow$ When predicting label, you can assign higher weight to the examples which are closer to the query example.  \n",
    "- Exercise for you: Play around with this argument. Do you get a better validation score? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression with $k$-nearest neighbours ($k$-NNs)\n",
    "\n",
    "- Can we solve regression problems with $k$-nearest neighbours algorithm? \n",
    "- In $k$-NN regression we take the average of the $k$-nearest neighbours. \n",
    "- We can also have weighted regression. \n",
    "\n",
    "See an example of regression in the lecture notes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "n = 50\n",
    "X = np.linspace(-1, 1, n) + np.random.randn(n) * 0.01\n",
    "X = X[:, None]\n",
    "y = np.random.randn(n, 1) + X * 5\n",
    "y = y.ravel()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=123\n",
    ")\n",
    "grid = np.linspace(min(X_train), max(X_train), 1000)\n",
    "\n",
    "plt.plot(X_train, y_train, \".\", markersize=10)\n",
    "plt.xlabel(\"X_train\")\n",
    "plt.ylabel(\"y_train\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knnr = KNeighborsRegressor(n_neighbors=1, weights=\"uniform\")\n",
    "knnr.fit(X_train, y_train)\n",
    "plt.plot(X_train, y_train, \".\", markersize=10)\n",
    "plt.plot(grid, knnr.predict(grid))\n",
    "plt.xlabel(\"feature\")\n",
    "plt.ylabel(\"target\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "### `score` method for regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "knnr.score(\n",
    "    X_train, y_train\n",
    ")  # Returns the coefficient of determination R^2 of the prediction (not accuracy!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "And with $k=10$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "knnr = KNeighborsRegressor(n_neighbors=10, weights=\"uniform\")\n",
    "knnr.fit(X_train, y_train)\n",
    "plt.plot(X_train, y_train, \".\", markersize=10)\n",
    "plt.plot(grid, knnr.predict(grid))\n",
    "plt.xlabel(\"feature\")\n",
    "plt.ylabel(\"target\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "knnr.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Using weighted distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "knnr = KNeighborsRegressor(n_neighbors=10, weights=\"distance\")\n",
    "knnr.fit(X_train, y_train)\n",
    "plt.plot(X_train, y_train, \".\", markersize=10)\n",
    "plt.plot(grid, knnr.predict(grid))\n",
    "knnr.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Summary of $k$-NN for supervised learning\n",
    "\n",
    "Pros:\n",
    "\n",
    "- Easy to understand, interpret.\n",
    "- Simple hyperparameter $k$ (`n_neighbors`) controlling the fundamental tradeoff.\n",
    "- Can learn very complex functions given enough data.\n",
    "- Lazy learning: Takes no time to `fit`\n",
    "\n",
    "Cons:\n",
    "\n",
    "- Can be potentially be VERY slow during prediction time, especially when the training set is very large. \n",
    "- Often not that great test accuracy compared to the modern approaches.\n",
    "- It does not work well on datasets with many features or where most feature values are 0 most of the time (sparse datasets).    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Attention\n",
    ":class: important\n",
    "\n",
    "For regular $k$-NN for supervised learning (not with sparse matrices), you should scale your features. We'll be looking into it soon. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parametric vs non parametric \n",
    "\n",
    "- You might see a lot of definitions of these terms.\n",
    "- A simple way to think about this is: \n",
    "    - do you need to store at least $O(n)$ worth of stuff to make predictions? If so, it's non-parametric.\n",
    "- Non-parametric example: $k$-NN is a classic example of non-parametric models.     \n",
    "- Parametric example: decision stump\n",
    "- If you want to know more about this, find some reading material [here](https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/L6.pdf), [here](http://mlss.tuebingen.mpg.de/2015/slides/ghahramani/gp-neural-nets15.pdf), and [here](https://machinelearningmastery.com/parametric-and-nonparametric-machine-learning-algorithms/). \n",
    "- By the way, the terms \"parametric\" and \"non-paramteric\" are often used differently by statisticians, see [here](https://help.xlstat.com/s/article/what-is-the-difference-between-a-parametric-and-a-nonparametric-test?language=en_US) for more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Curse of dimensionality\n",
    "\n",
    "- Affects all learners but especially bad for nearest-neighbour. \n",
    "- $k$-NN usually works well when the number of dimensions $d$ is small but things fall apart quickly as $d$ goes up.\n",
    "- If there are many irrelevant attributes, $k$-NN is hopelessly confused because all of them contribute to finding similarity between examples. \n",
    "- With enough irrelevant attributes the accidental similarity swamps out meaningful similarity and $k$-NN is no better than random guessing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "nfeats_accuracy = {\"nfeats\": [], \"dummy_valid_accuracy\": [], \"KNN_valid_accuracy\": []}\n",
    "for n_feats in range(4, 2000, 100):\n",
    "    X, y = make_classification(n_samples=2000, n_features=n_feats, n_classes=2)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=123\n",
    "    )\n",
    "    dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "    dummy_scores = cross_validate(dummy, X_train, y_train, return_train_score=True)\n",
    "\n",
    "    knn = KNeighborsClassifier()\n",
    "    scores = cross_validate(knn, X_train, y_train, return_train_score=True)\n",
    "    nfeats_accuracy[\"nfeats\"].append(n_feats)\n",
    "    nfeats_accuracy[\"KNN_valid_accuracy\"].append(np.mean(scores[\"test_score\"]))\n",
    "    nfeats_accuracy[\"dummy_valid_accuracy\"].append(np.mean(dummy_scores[\"test_score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(nfeats_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### $k$-NN True/False questions\n",
    "\n",
    "1. Unlike with decision trees, with $k$-NNs most of the work is done at the `predict` stage. \n",
    "2. With $k$-NN, setting the hyperparameter $k$ to larger values typically reduces training error. \n",
    "3. $k$-NN may perform poorly in high-dimensional space (say, *d* > 100). \n",
    "4. Similar to decision trees, $k$-NNs finds a small set of good features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines (SVMs) with RBF kernel \n",
    "- Very high-level overview\n",
    "- Our goals here are\n",
    "    - Use `scikit-learn`'s SVM model. \n",
    "    - Explain the similarities and differences between $k$-NNs and SVMs with RBF kernel\n",
    "    - Explain the notion of support vectors\n",
    "    - Explain how `C` and `gamma` hyperparameters control the fundamental tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Another popular similarity-based algorithm is Support Vector Machines (SVM with RBFs)\n",
    "- Superficially, SVMs are more like weighted $k$-NNs.\n",
    "    - The decision boundary is defined by **a set of positive and negative examples** and **their weights** together with **their similarity measure**. \n",
    "    - A test example is a positive if on average it looks more like positive examples than the negative examples. \n",
    "- The primary difference between $k$-NNs and SVMs is that \n",
    "    - Unlike $k$-NNs, SVMs only remember the key examples (support vectors). So it's more efficient than $k$-NN. \n",
    "    - SVMs use a different similarity metric which is called a \"kernel\" in SVM land. A popular kernel is Radial Basis Functions (RBFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(cities_df, test_size=0.2, random_state=123)\n",
    "canada = train_df.query('country == \"Canada\"')\n",
    "usa = train_df.query('country == \"USA\"')\n",
    "plt.scatter(canada[\"longitude\"], canada[\"latitude\"], color=\"red\", alpha=0.6)\n",
    "plt.scatter(usa[\"longitude\"], usa[\"latitude\"], color=\"blue\", alpha=0.6)\n",
    "plt.ylabel(\"latitude\")\n",
    "plt.xlabel(\"longitude\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# split into training/validation and testing set\n",
    "X_train, y_train = train_df.drop(columns=[\"country\"]), train_df[\"country\"]\n",
    "X_test, y_test = test_df.drop(columns=[\"country\"]), test_df[\"country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "scores = cross_validate(knn, X_train, y_train, return_train_score=True)\n",
    "print(\"Mean validation score %0.3f\" % (np.mean(scores[\"test_score\"])))\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(gamma=0.01)  # Ignore gamma for now\n",
    "svm.fit(X_train, y_train)\n",
    "scores = cross_validate(svm, X_train, y_train, return_train_score=True)\n",
    "print(\"Mean validation score %0.3f\" % (np.mean(scores[\"test_score\"])))\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# You can think of SVM with RBF kernel as \"smooth KNN\"\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"SVC\")\n",
    "plot_classifier(X_train, y_train, svm, ax=plt.gca())\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"KNN with k = 5\")\n",
    "plot_classifier(X_train, y_train, knn, ax=plt.gca())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(\"KNN test score: %0.3f\" % (knn.score(X_test, y_test)))\n",
    "print(\"SVM test score: %0.3f\" % (svm.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Support vectors \n",
    "\n",
    "- Each training example either is or isn't a \"support vector\".\n",
    "  - This gets decided during `fit`.\n",
    "\n",
    "- **Main insight: the decision boundary only depends on the support vectors.**\n",
    "\n",
    "- Let's look at the support vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Let's generate some fake data\n",
    "# generate blobs with fixed random generator\n",
    "n = 20\n",
    "n_classes = 2\n",
    "X_toy, y_toy = make_blobs(n_samples=n, centers=n_classes, random_state=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Let's train an SVM classifier.\n",
    "svm = SVC()\n",
    "svm.fit(X_toy, y_toy)\n",
    "plt.title(\"SVM\")\n",
    "plot_classifier(X_toy, y_toy, svm, ax=plt.gca())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# How do we access support vectors\n",
    "sv = svm.support_\n",
    "not_sv = list(set(range(n)) - set(sv))\n",
    "print(\"Support vectors: \", sv)\n",
    "print(\"Non support vectors: \", not_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plot_classifier(X_toy, y_toy, svm, ax=plt.gca())\n",
    "plt.scatter(\n",
    "    *svm.support_vectors_.T, marker=\"o\", edgecolor=\"yellow\", facecolor=\"none\", s=120\n",
    ")\n",
    "# The support vectors (SVs) are shown in yellow.\n",
    "# These are the example that \"support\" the boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Support vectors\n",
    "\n",
    "- Note that the number of support vectors is smaller compared to the training set. \n",
    "    - Makes a big difference on large datasets. \n",
    "- What happens if we delete all non-support vector? Would the decision boundary change? \n",
    "- What happens if we delete a support vector? Would the decision boundary change? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's try removing all other examples, keeping only the SVs.\n",
    "# remove all non-support vectors\n",
    "X_only_SVs = np.delete(X_toy, not_sv, 0)\n",
    "y_only_SVs = np.delete(y_toy, not_sv, 0)\n",
    "svm_only_SVs = SVC()\n",
    "svm_only_SVs.fit(X_only_SVs, y_only_SVs)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_classifier(X_toy, y_toy, svm, ax=plt.gca())\n",
    "plt.scatter(\n",
    "    *svm.support_vectors_.T, marker=\"o\", edgecolor=\"yellow\", facecolor=\"none\", s=120\n",
    ")\n",
    "plt.title(\"Original\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_classifier(X_only_SVs, y_only_SVs, svm_only_SVs, ax=plt.gca())\n",
    "plt.scatter(\n",
    "    *svm_only_SVs.support_vectors_.T,\n",
    "    marker=\"o\",\n",
    "    edgecolor=\"yellow\",\n",
    "    facecolor=\"none\",\n",
    "    s=120\n",
    ")\n",
    "plt.title(\"SVs only\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_remove_SV = np.delete(X_toy, sv[1], 0)\n",
    "y_remove_SV = np.delete(y_toy, sv[1], 0)\n",
    "\n",
    "svm_remove_SV = SVC()\n",
    "svm_remove_SV.fit(X_remove_SV, y_remove_SV)\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_classifier(X_toy, y_toy, svm, ax=plt.gca())\n",
    "plt.scatter(\n",
    "    *svm.support_vectors_.T, marker=\"o\", edgecolor=\"yellow\", facecolor=\"none\", s=120\n",
    ")\n",
    "plt.title(\"Original\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_classifier(X_remove_SV, y_remove_SV, svm_remove_SV, ax=plt.gca())\n",
    "plt.scatter(\n",
    "    *svm_remove_SV.support_vectors_.T,\n",
    "    marker=\"o\",\n",
    "    edgecolor=\"yellow\",\n",
    "    facecolor=\"none\",\n",
    "    s=120\n",
    ")\n",
    "plt.title(\"With one SV removed\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hyperparameters of SVM \n",
    "\n",
    "- Key hyperparameters of `rbf` SVM are\n",
    "    - `gamma`\n",
    "    - `C`\n",
    "    \n",
    "- We are not equipped to understand the meaning of these parameters at this point but you are expected to describe their relation to the fundamental tradeoff. \n",
    "\n",
    "See [`scikit-learn`'s explanation of RBF SVM parameters](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Relation of `gamma` and the fundamental trade-off\n",
    "\n",
    "- `gamma` controls the complexity (fundamental trade-off), just like other hyperparameters we've seen.\n",
    "  - larger `gamma` $\\rightarrow$ more complex\n",
    "  - smaller `gamma` $\\rightarrow$ less complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    gamma = 10.0 ** (i - 3)\n",
    "    rbf_svm = SVC(gamma=gamma)\n",
    "    rbf_svm.fit(X_train, y_train)\n",
    "    plot_classifier(X_train, y_train, rbf_svm, ax=plt.gca(), show_data=False)\n",
    "    plt.title(\"gamma = %s\" % gamma);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Relation of `C` and the fundamental trade-off\n",
    "\n",
    "- `C` _also_ affects the fundamental tradeoff\n",
    "    - larger `C` $\\rightarrow$ more complex \n",
    "    - smaller `C` $\\rightarrow$ less complex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    C = 10.0 ** (i - 1)\n",
    "    rbf_svm = SVC(C=C, gamma=0.01)\n",
    "    rbf_svm.fit(X_train, y_train)\n",
    "    plot_classifier(X_train, y_train, rbf_svm, ax=plt.gca(), show_data=False)\n",
    "    plt.title(\"C = %s\" % C);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SVM RBF True/False questions\n",
    "\n",
    "1. Similar to KNN, SVM with RBF kernel is a non-parametric model.\n",
    "2. In SVM RBF, removing a non-support vector does not change the decision boundary. \n",
    "3. In sklearnâ€™s SVC classifier, large values of gamma tend to result in higher training score but probably lower validation score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Search over multiple hyperparameters\n",
    "\n",
    "- So far you have seen how to carry out search over a hyperparameter\n",
    "- In the above case the best training error is achieved by the most complex model (large `gamma`, large `C`).\n",
    "- Best validation error requires a hyperparameter search to balance the fundamental tradeoff.\n",
    "  - In general we can't search them one at a time.\n",
    "  - More on this next week. But if you cannot wait till then, you may look up the following:\n",
    "    - [sklearn.model_selection.GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "    - [sklearn.model_selection.RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### More on RBF SVM and interpretation of `gamma` and `C` in 573! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SVM Regressor\n",
    "\n",
    "- Similar to KNNs, you can use SVMs for regression problems as well.\n",
    "- See [`sklearn.svm.SVR`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) for more details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- We have KNNs and SVMs as new supervised learning techniques in our toolbox.\n",
    "- These are analogy-based learners and the idea is to assign nearby points the same label.\n",
    "- Unlike decision trees, all features are equally important. \n",
    "- Both can be used for classification or regression (much like the other methods we've seen)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Revisit: Lecture learning objectives\n",
    "\n",
    "From this lecture, you will be able to \n",
    "\n",
    "- explain the notion of similarity-based algorithms; \n",
    "- broadly describe how $k$-NNs use distances; \n",
    "- discuss the effect of using a small/large value of the hyperparameter $k$ when using the $k$-NN algorithm; \n",
    "- describe the problem of curse of dimensionality; \n",
    "- explain the general idea of SVMs with RBF kernel; \n",
    "- explain the differences between $k$-NNs and SVM RBFs;\n",
    "- broadly describe the relation of `gamma` and `C` hyperparameters with the fundamental tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Questions for group discussion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "1. When we calculated Euclidean distances from all cities to all other cities, why did we set the diagonal entries to infinity before finding the closest city?\n",
    "2. Why do we have to treat $k$ as a hyperparameter rather than a parameter? \n",
    "3. Which of the following models are parametric and which ones are non-parametric? \n",
    "    - Decision stumps\n",
    "    - decision trees with no depth\n",
    "    - KNNs\n",
    "    - SVMs with RBF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### KNN practice question\n",
    "\n",
    "Consider this toy dataset:\n",
    "\n",
    "$$ X = \\begin{bmatrix}5 & 2\\\\4 & 3\\\\  2 & 2\\\\ 10 & 10\\\\ 9 & -1\\\\ 9& 9\\end{bmatrix}, \\quad y = \\begin{bmatrix}0\\\\0\\\\1\\\\1\\\\1\\\\2\\end{bmatrix}.$$\n",
    "\n",
    "1. If $k=1$, what would you predict for $x=\\begin{bmatrix} 0\\\\0\\end{bmatrix}$?\n",
    "2. If $k=3$, what would you predict for $x=\\begin{bmatrix} 0\\\\0\\end{bmatrix}$?\n",
    "3. If $k=3$, what would you predict for $x=\\begin{bmatrix} 0\\\\0\\end{bmatrix}$ if we were doing regression rather than classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Varada's solution\n",
    "\n",
    "X = pd.DataFrame({\"feature1\": [5, 4, 2, 10, 9, 9], \"feature2\": [2, 3, 2, 10, -1, 9]})\n",
    "y = [0, 0, 1, 1, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn1 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn1.fit(X, y)\n",
    "knn1.predict([[0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn3 = KNeighborsClassifier(n_neighbors=3)\n",
    "knn3.fit(X, y)\n",
    "knn3.predict([[0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnr = KNeighborsRegressor(n_neighbors=3)\n",
    "knnr.fit(X, y)\n",
    "knnr.predict([[0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
