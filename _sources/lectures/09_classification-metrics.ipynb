{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Lecture 9: Classification Metrics\n",
    "\n",
    "UBC 2020-21\n",
    "\n",
    "Instructor: Varada Kolhatkar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"code/.\")\n",
    "\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "from plotting_functions import *\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Changing global matplotlib settings for confusion matrix.\n",
    "plt.rcParams[\"xtick.labelsize\"] = 18\n",
    "plt.rcParams[\"ytick.labelsize\"] = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lecture plan\n",
    "\n",
    "**A reminder to start recording.**\n",
    "\n",
    "- Motivation (~5 mins)\n",
    "- Watch confusion matrix and precision/recall videos (~17 mins)\n",
    "- Poll on the videos (~10 mins)\n",
    "- Break (~5 mins)\n",
    "- Live lecturing: PR Curve and ROC curve (~20 mins)\n",
    "- Live lecturing: Handling data imbalance (~10 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Announcements\n",
    "\n",
    "- HW4 is due tomorrow. \n",
    "- HW5 will be released next week Monday. \n",
    "- Midterm is coming up soon (Oct 28th during class time)\n",
    "- All Zoom lectures are being recorded. You can find the recording under \n",
    "- I'll let you know this weekend whether we are doing in person classes or Zoom classes next week. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Learning outcomes \n",
    "\n",
    "From this lecture, students are expected to be able to:\n",
    "\n",
    "- Explain why accuracy is not always the best metric in ML.\n",
    "- Explain components of a confusion matrix. \n",
    "- Define precision, recall, and f1-score and use them to evaluate different classifiers. \n",
    "- Broadly explain macro-average, weighted average.\n",
    "- Interpret and use precision-recall curves. \n",
    "- Explain average precision score.\n",
    "- Interpret and use ROC curves and ROC AUC using `scikit-learn`.  \n",
    "- Identify whether there is class imbalance and whether you need to deal with it.\n",
    "- Explain and use `class_weight` to deal with data imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation metrics for binary classification: Motivation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataset for demonstration \n",
    "\n",
    "- Let's classify fraudulent and non-fraudulent transactions using Kaggle's [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud) data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cc_df = pd.read_csv(\"data/creditcard.csv\", encoding=\"latin-1\")\n",
    "train_df, test_df = train_test_split(cc_df, test_size=0.3, random_state=111)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Good size dataset \n",
    "- For confidentially reasons, it only provides transformed features with PCA, which is a popular dimensionality reduction technique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- We do not have categorical features. All features are numeric. \n",
    "- We have to be careful about the `Time` and `Amount` features. \n",
    "- We could scale `Amount`. \n",
    "- Do we want to scale time?\n",
    "    - In this lecture we'll do it's probably not the best thing to do. \n",
    "    - We'll learn about time series briefly later in the course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's separate `X` and `y` for train and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train_big, y_train_big = train_df.drop(columns=[\"Class\"]), train_df[\"Class\"]\n",
    "X_test, y_test = test_df.drop(columns=[\"Class\"]), test_df[\"Class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- It's easier to demonstrate evaluation metrics using an explicit validation set instead of using cross-validation. \n",
    "- So let's create a validation set. \n",
    "- Our data is large enough so it shouldn't be a problem. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_big, y_train_big, test_size=0.3, random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier()\n",
    "pd.DataFrame(cross_validate(dummy, X_train, y_train, return_train_score=True)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Observations \n",
    "\n",
    "- `DummyClassifier` is getting 0.998 cross-validation accuracy!! \n",
    "- Should we be happy with this accuracy and deploy this `DummyClassifier` model for fraud detection? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "What's the class distribution? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df[\"Class\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We have class imbalance. \n",
    "- We have MANY non-fraud transactions and only a handful of fraud transactions. \n",
    "- So in the training set, `most_frequent` strategy is labeling 199,025 (99.83%) instances correctly and only 339 (0.17%) instances incorrectly. \n",
    "- Is this what we want? \n",
    "- The \"fraud\" class is the important class that we want to spot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's scale the features and try `LogisticRegression`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pd.DataFrame(cross_validate(pipe, X_train, y_train, return_train_score=True)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- We are getting a slightly better score with logistic regression.  \n",
    "- What score should be considered an acceptable score here? \n",
    "- Are we actually spotting any \"fraud\" transactions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- `.score` by default returns accuracy which is \n",
    "$$\\frac{\\text{correct predictions}}{\\text{total examples}}$$\n",
    "- Is accuracy a good metric here? \n",
    "- Is there anything more informative than accuracy that we can use here? \n",
    "\n",
    "Let's dig a little deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One way to get a better understanding of the errors is by looking at \n",
    "- false positives (type I errors), where the model incorrectly spots examples as fraud\n",
    "- false negatives (type II errors), where it's missing to spot fraud examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "disp = plot_confusion_matrix(\n",
    "    pipe,\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    display_labels=[\"Non fraud\", \"fraud\"],\n",
    "    values_format=\"d\",\n",
    "    cmap=plt.cm.Blues,\n",
    "    colorbar=False,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predictions = pipe.predict(X_valid)\n",
    "TN, FP, FN, TP = confusion_matrix(y_valid, predictions).ravel()\n",
    "plot_confusion_matrix_example(TN, FP, FN, TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Perfect prediction has all values down the diagonal\n",
    "- Off diagonal entries can often tell us about what is being mis-predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is \"positive\" and \"negative\"?\n",
    "\n",
    "- Two kinds of binary classification problems \n",
    "    - Distinguishing between two classes\n",
    "    - Spotting a class (spot fraud transaction, spot spam, spot disease)\n",
    "- In case of spotting problems, the thing that we are interested in spotting is considered \"positive\". \n",
    "- Above we wanted to spot fraudulent transactions and so they are \"positive\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can get a numpy array of confusion matrix as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predictions = pipe.predict(X_valid)\n",
    "TN, FP, FN, TP = confusion_matrix(y_valid, predictions).ravel()\n",
    "print(\"Confusion matrix for fraud data set\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confusion matrix with cross-validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- You can also calculate confusion matrix with cross-validation using the `cross_val_predict` method.  \n",
    "- But then you cannot conveniently use `plot_confusion_matrix`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "confusion_matrix(y_train, cross_val_predict(pipe, X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Precision, recall, f1 score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- We have been using `.score` to assess our models, which returns accuracy by default. \n",
    "- Accuracy is misleading when we have class imbalance.\n",
    "- We need other metrics to assess our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We'll discuss three commonly used metrics which are based on confusion matrix: \n",
    "    - recall\n",
    "    - precision\n",
    "    - f1 score \n",
    "- Note that these metrics will only help us assessing our model.  \n",
    "- Later we'll talk about a few ways to address class imbalance problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pipe_lr = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "predictions = pipe_lr.predict(X_valid)\n",
    "TN, FP, FN, TP = confusion_matrix(y_valid, predictions).ravel()\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall \n",
    "\n",
    "Among all positive examples, how many did you identify?\n",
    "$$ recall = \\frac{TP}{TP+FN} = \\frac{TP}{\\#positives} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    pipe,\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    display_labels=[\"Non fraud\", \"fraud\"],\n",
    "    values_format=\"d\",\n",
    "    cmap=plt.cm.Blues,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TP = %0.4f, FN = %0.4f\" % (TP, FN))\n",
    "recall = TP / (TP + FN)\n",
    "print(\"Recall: %0.4f\" % (recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision \n",
    "\n",
    "Among the positive examples you identified, how many were actually positive?\n",
    "\n",
    "$$ precision = \\frac{TP}{TP+FP}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    pipe,\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    display_labels=[\"Non fraud\", \"fraud\"],\n",
    "    values_format=\"d\",\n",
    "    cmap=plt.cm.Blues,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TP = %0.4f, FP = %0.4f\" % (TP, FP))\n",
    "precision = TP / (TP + FP)\n",
    "print(\"Precision: %0.4f\" % (precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### F1-score\n",
    "\n",
    "- F1-score combines precision and recall to give one score, which could be used in hyperparameter optimization, for instance. \n",
    "- F1-score is a harmonic mean of precision and recall. \n",
    "\n",
    "\n",
    "$$ f1 = 2 \\times \\frac{ precision \\times recall}{precision + recall}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"precision: %0.4f\" % (precision))\n",
    "print(\"recall: %0.4f\" % (recall))\n",
    "f1_score = (2 * precision * recall) / (precision + recall)\n",
    "print(\"f1: %0.4f\" % (f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at all metrics at once on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## Calculate evaluation metrics by ourselves\n",
    "data = {\n",
    "    \"calculation\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"error\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f1 score\": [],\n",
    "}\n",
    "data[\"calculation\"].append(\"manual\")\n",
    "data[\"accuracy\"].append((TP + TN) / (TN + FP + FN + TP))\n",
    "data[\"error\"].append((FP + FN) / (TN + FP + FN + TP))\n",
    "data[\"precision\"].append(precision)  # TP / (TP + FP)\n",
    "data[\"recall\"].append(recall)  # TP / (TP + FN)\n",
    "data[\"f1 score\"].append(f1_score)  # (2 * precision * recall) / (precision + recall)\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- `scikit-learn` has functions for [these metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "data[\"accuracy\"].append(accuracy_score(y_valid, pipe_lr.predict(X_valid)))\n",
    "data[\"error\"].append(1 - accuracy_score(y_valid, pipe_lr.predict(X_valid)))\n",
    "data[\"precision\"].append(\n",
    "    precision_score(y_valid, pipe_lr.predict(X_valid), zero_division=1)\n",
    ")\n",
    "data[\"recall\"].append(recall_score(y_valid, pipe_lr.predict(X_valid)))\n",
    "data[\"f1 score\"].append(f1_score(y_valid, pipe_lr.predict(X_valid)))\n",
    "data[\"calculation\"].append(\"sklearn\")\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index([\"calculation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores match. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is a convenient function called `classification_report` in `sklearn` which gives this info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        y_valid, pipe_lr.predict(X_valid), target_names=[\"non-fraud\", \"fraud\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Macro average\n",
    "\n",
    "- You give equal importance to all classes and average over all classes.  \n",
    "- For instance, in the example above, recall for non-fraud is 1.0 and fraud is 0.63, and so macro average is 0.81. \n",
    "- More relevant in case of multi-class problems.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weighted average\n",
    "\n",
    "- Weighted by the number of samples in each class. \n",
    "- Divide by the total number of samples. \n",
    "\n",
    "Which one is relevant when depends upon whether you think each class should have the same weight or each sample should have the same weight. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interim summary \n",
    "\n",
    "- Accuracy is misleading when you have class imbalance. \n",
    "- A confusion matrix provides a way to break down errors made by our model. \n",
    "- We looked at three metrics based on confusion matrix: \n",
    "    - precision, recall, f1-score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Note that what you consider \"positive\" (fraud in our case) is important when calculating precision, recall, and f1-score. \n",
    "- If you flip what is considered positive or negative, we'll end up with different TP, FP, TN, FN, and hence different precision, recall, and f1-scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evalution metrics overview  \n",
    "There is a lot of terminology here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='./img/evaluation-metrics.png' width=\"1000\" height=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross validation with different metrics\n",
    "\n",
    "- We can pass different evaluation metrics with `scoring` argument of `cross_validate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "scoring = [\n",
    "    \"accuracy\",\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\",\n",
    "]  # scoring can be a string, a list, or a dictionary\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "scores = cross_validate(\n",
    "    pipe, X_train_big, y_train_big, return_train_score=True, scoring=scoring\n",
    ")\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- You can also create [your own scoring function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) and pass it to `cross_validate`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ❓❓ Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### True/False questions: decision theory, evaluation metrics\n",
    "\n",
    "1. In medical diagnosis, false positives are more damaging than false negatives (assume \"positive\" means the person has a disease, \"negative\" means they don't).\n",
    "2. In spam classification, false positives are more damaging than false negatives (assume \"positive\" means the email is spam, \"negative\" means they it's not).\n",
    "3. In the medical diagnosis, high recall is more important than high precision.\n",
    "4. If method A gets a higher accuracy than method B, that means its precision is also higher.\n",
    "5. If method A gets a higher accuracy than method B, that means its recall is also higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Method A - higher accuracy but lower precision\n",
    "\n",
    "| Negative | Positive\n",
    "| -------- |:-------------:|\n",
    "| 90      | 5|\n",
    "| 5      | 0|\n",
    "\n",
    "Method B - lower accuracy but higher precision\n",
    "\n",
    "| Negative | Positive\n",
    "| -------- |:-------------:|\n",
    "| 80      | 15|\n",
    "| 0      | 5|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Precision-recall curve and ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Confusion matrix provides a detailed break down of the errors made by the model. \n",
    "- But when creating a confusion matrix, we are using \"hard\" predictions. \n",
    "- Most classifiers in `scikit-learn` provide `predict_proba` method (or `decision_function`) which provides degree of certainty about predictions by the classifier. \n",
    "- Can we explore the degree of uncertainty to understand and improve the model performance? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's revisit the classification report on our fraud detection example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe_lr.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = pipe_lr.predict(X_valid)\n",
    "print(classification_report(y_valid, y_pred, target_names=[\"non-fraud\", \"fraud\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By default, predictions use the threshold of 0.5. If `predict_proba` > 0.5, predict \"fraud\" else predict \"non-fraud\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = pipe_lr.predict_proba(X_valid)[:, 1] > 0.50\n",
    "print(classification_report(y_valid, y_pred, target_names=[\"non-fraud\", \"fraud\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Suppose for your business it is more costly to miss fraudulent transactions and you want to achieve a recall of at least 75% for the \"fraud\" class. \n",
    "- One way to do this is by changing the threshold of `predict_proba`.\n",
    "    - `predict` returns 1 when `predict_proba`'s probabilities are above 0.5 for the \"fraud\" class.\n",
    "\n",
    "**Key idea: what if we threshold the probability at a smaller value so that we identify more examples as \"fraud\" examples?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's lower the threshold to 0.1. In other words, predict the examples as \"fraud\" if `predict_proba` > 0.1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lower_threshold = pipe_lr.predict_proba(X_valid)[:, 1] > 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_valid, y_pred_lower_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Operating point \n",
    "\n",
    "- Now our recall for \"fraud\" class is >= 0.75. \n",
    "- Setting a requirement on a classifier (e.g., recall of >= 0.75) is called setting the **operating point**. \n",
    "- It's usually driven by business goals and is useful to make performance guarantees to customers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision/Recall tradeoff \n",
    "\n",
    "- But there is a trade-off between precision and recall. \n",
    "- If you identify more things as \"fraud\", recall is going to increase but there are likely to be more false positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's sweep through different thresholds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "thresholds = np.arange(0.0, 1.0, 0.1)\n",
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pr_dict = {\"threshold\": [], \"precision\": [], \"recall\": [], \"f1 score\": []}\n",
    "for threshold in thresholds:\n",
    "    preds = pipe_lr.predict_proba(X_valid)[:, 1] > threshold\n",
    "    pr_dict[\"threshold\"].append(threshold)\n",
    "    pr_dict[\"precision\"].append(precision_score(y_valid, preds))\n",
    "    pr_dict[\"recall\"].append(recall_score(y_valid, preds))\n",
    "    pr_dict[\"f1 score\"].append(f1_score(y_valid, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pr_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decreasing the threshold\n",
    "\n",
    "- Decreasing the threshold means a lower bar for predicting fraud. \n",
    "    - You are willing to risk more false positives in exchange of more true positives. \n",
    "    - recall would either stay the same or go up and precision is likely to go down\n",
    "    - occasionally, precision may increase if all the new examples after decreasing the threshold are TPs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Increasing the threshold\n",
    "\n",
    "- Increasing the threshold means a higher bar for predicting fraud. \n",
    "    - recall would go down or stay the same but precision is likely to go up \n",
    "    - occasionally, precision may go down as the denominator for precision is TP+FP.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision-recall curve\n",
    "\n",
    "Often, when developing a model, it's not always clear what the operating point will be and to understand the the model better, it's informative to look at all possible thresholds and corresponding trade-offs of precision and recall in a plot.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_valid, pipe_lr.predict_proba(X_valid)[:, 1]\n",
    ")\n",
    "plt.plot(precision, recall, label=\"logistic regression: PR curve\")\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.plot(\n",
    "    precision_score(y_valid, pipe_lr.predict(X_valid)),\n",
    "    recall_score(y_valid, pipe_lr.predict(X_valid)),\n",
    "    \"or\",\n",
    "    markersize=10,\n",
    "    label=\"threshold 0.5\",\n",
    ")\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Each point in the curve corresponds to a possible threshold of the `predict_proba` output. \n",
    "- We can achieve a recall of 0.8 at a precision of 0.4. \n",
    "- The red dot marks the point corresponding to the threshold 0.5.\n",
    "- The top-right would be a perfect classifier (precision = recall = 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The threshold is not shown here, but it's going from 0 (upper-left) to 1 (lower right).\n",
    "- At a threshold of 0 (upper left), we are classifying everything  as \"fraud\".\n",
    "- Raising the threshold increases the precision but at the expense of lowering the recall. \n",
    "- At the extreme right, where the threshold is 1, we get into the situation where all the examples classified as \"fraud\" are actually \"fraud\"; we have no false positives. \n",
    "- Here we have a high precision but lower recall. \n",
    "- Usually the goal is to keep recall high as precision goes up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A few comments on PR curve\n",
    "\n",
    "- Different classifiers might work well in different parts of the curve, i.e., at different operating points.   \n",
    "- We can compare PR curves of different classifiers to understand these differences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AP score \n",
    "\n",
    "- Often it's useful to have one number summarizing the PR plot (e.g., in hyperparameter optimization)\n",
    "- One way to do this is by computing the area under the PR curve. \n",
    "- This is called **average precision** (AP score)\n",
    "- AP score has a value between 0 (worst) and 1 (best). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "ap_lr = average_precision_score(y_valid, pipe_lr.predict_proba(X_valid)[:, 1])\n",
    "print(\"Average precision of logistic regression: {:.3f}\".format(ap_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AP vs. F1-score\n",
    "\n",
    "It is very important to note this distinction:\n",
    "\n",
    "- F1 score is for a given threshold and measures the quality of `predict`.\n",
    "- AP score is a summary across thresholds and measures the quality of `predict_proba`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{important}\n",
    "Remember to pick the desired threshold based on the results on the validation set and **not** on the test set.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Receiver Operating Characteristic (ROC) curve \n",
    "\n",
    "- Another commonly used tool to analyze the behavior of classifiers at different thresholds.  \n",
    "- Similar to PR curve, it considers all possible thresholds for a given classifier given by `predict_proba` but instead of precision and recall it plots false positive rate (FPR) and true positive rate (TPR or recall).\n",
    "$$ FPR  = \\frac{FP}{TP + FP}, TPR = \\frac{TP}{TP + FP}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_valid, pipe_lr.predict_proba(X_valid)[:, 1])\n",
    "plt.plot(fpr, tpr, label=\"ROC Curve\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR (recall)\")\n",
    "\n",
    "default_threshold = np.argmin(np.abs(thresholds - 0.5))\n",
    "\n",
    "plt.plot(\n",
    "    fpr[default_threshold],\n",
    "    tpr[default_threshold],\n",
    "    \"or\",\n",
    "    markersize=10,\n",
    "    label=\"threshold 0.5\",\n",
    ")\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The ideal curve is close to the top left\n",
    "    - Ideally, you want a classifier with high recall while keeping low false positive rate.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The red dot corresponds to the threshold of 0.5, which is used by predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Area under the curve (AUC)\n",
    "\n",
    "- AUC provides a single meaningful number for the model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_lr = roc_auc_score(y_valid, pipe_lr.predict_proba(X_valid)[:, 1])\n",
    "print(\"AUC for SVC: {:.3f}\".format(roc_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- AUC of 0.5 means random chance. \n",
    "- AUC can be interpreted as evaluating the **ranking** of positive examples.\n",
    "- What's the probability that a randomly picked positive point has a higher score according to the classifier than a randomly picked point from the negative class. \n",
    "- AUC of 1.0 means all positive points have a higher score than all negative points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{important}\n",
    "For classification problems with imbalanced classes, using AP score or AUC is often much more meaningful than using accuracy. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's look at all the scores at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = [\"accuracy\", \"f1\", \"recall\", \"precision\", \"roc_auc\", \"average_precision\"]\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "scores = cross_validate(pipe, X_train_big, y_train_big, scoring=scoring)\n",
    "pd.DataFrame(scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{seealso}\n",
    "Check out [these visualization](https://github.com/dariyasydykova/open_projects/tree/master/ROC_animation) on ROC and AUC.  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dealing with class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Class imbalance in training sets\n",
    "\n",
    "- This typically refers to having many more examples of one class than another in one's training set.\n",
    "- Real world data is often imbalanced. \n",
    "    - Our Credit Card Fraud dataset is imbalanced.\n",
    "    - Ad clicking data is usually drastically imbalanced. (Only around ~0.01% ads are clicked.)\n",
    "    - Spam classification datasets are also usually imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Addressing class imbalance\n",
    "A very important question to ask yourself: \"Why do I have a class imbalance?\"\n",
    "\n",
    "- Is it because one class is much more rare than the other?\n",
    "    - If it's just because one is more rare than the other, you need to ask whether you care about one type of error more than the other.    \n",
    "- Is it because of my data collection methods?\n",
    "    - If it's the data collection, then that means _your test and training data come from different distributions_!\n",
    "  \n",
    "But, if you answer \"no\" to both question, it may be fine to just ignore the class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Which type of error is more important? \n",
    "\n",
    "- False positives (FPs) and false negatives (FNs) have quite different real-world consequences. \n",
    "- In PR curve and ROC curve, we saw how changing the prediction threshold can change FPs and FNs. \n",
    "- We can then pick the threshold that's appropriate for our problem. \n",
    "- Example: if we want high recall, we may use a lower threshold (e.g., a threshold of 0.1). We'll then catch more fraudulent transactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "y_pred = pipe_lr.predict(X_valid)\n",
    "print(classification_report(y_valid, y_pred, target_names=[\"non-fraud\", \"fraud\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = pipe_lr.predict_proba(X_valid)[:, 1] > 0.10\n",
    "print(classification_report(y_valid, y_pred, target_names=[\"non-fraud\", \"fraud\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Handling imbalance\n",
    "\n",
    "Can we change the model itself rather than changing the threshold so that it takes into account the errors that are important to us?\n",
    "\n",
    "There are two common approaches for this: \n",
    "- **Changing the training procedure** \n",
    "    - `class_weight`\n",
    "- **Changing the data (optional)** (not covered in this class but reach out if you're interested)\n",
    "   - Undersampling\n",
    "   - Oversampling \n",
    "       - Random oversampling\n",
    "       - SMOTE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Approach 1:  Changing the training procedure \n",
    "\n",
    "- All `sklearn` classifiers have a parameter called `class_weight`.\n",
    "- This allows you to specify that one class is more important than another.\n",
    "- For example, maybe a false negative is 10x more problematic than a false positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: `class_weight` parameter of `sklearn LogisticRegression` \n",
    "> class sklearn.linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, **class_weight=None**, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "\n",
    "> class_weight: dict or 'balanced', default=None\n",
    "\n",
    "> Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\"\n",
    "HTML(\"<iframe src=%s width=1000 height=650></iframe>\" % url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    pipe_lr,\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    display_labels=[\"Non fraud\", \"fraud\"],\n",
    "    values_format=\"d\",\n",
    "    cmap=plt.cm.Blues,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr.named_steps[\"logisticregression\"].classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's set \"fraud\" class a weight of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr_weight = make_pipeline(\n",
    "    StandardScaler(), LogisticRegression(max_iter=500, class_weight={1: 10})\n",
    ")\n",
    "pipe_lr_weight.fit(X_train, y_train)\n",
    "plot_confusion_matrix(\n",
    "    pipe_lr_weight,\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    display_labels=[\"Non fraud\", \"fraud\"],\n",
    "    values_format=\"d\",\n",
    "    cmap=plt.cm.Blues,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Notice we've reduced false negatives and predicted more Fraud this time.\n",
    "- This was equivalent to saying give 10x more \"importance\" to fraud class. \n",
    "- Note that as a consequence we are also increasing false positives.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `class_weight=\"balanced\"`\n",
    "- A useful setting is `class_weight=\"balanced\"`.\n",
    "- This sets the weights so that the classes are \"equal\".\n",
    "\n",
    "> class_weight: dict, ‘balanced’ or None\n",
    "If ‘balanced’, class weights will be given by n_samples / (n_classes * np.bincount(y)). If a dictionary is given, keys are classes and values are corresponding class weights. If None is given, the class weights will be uniform.\n",
    "\n",
    "> sklearn.utils.class_weight.compute_class_weight(class_weight, classes, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr_balanced = make_pipeline(\n",
    "    StandardScaler(), LogisticRegression(max_iter=500, class_weight=\"balanced\")\n",
    ")\n",
    "pipe_lr_balanced.fit(X_train, y_train)\n",
    "plot_confusion_matrix(\n",
    "    pipe_lr_balanced,\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    display_labels=[\"Non fraud\", \"fraud\"],\n",
    "    values_format=\"d\",\n",
    "    cmap=plt.cm.Blues,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reduced false negatives but we have many more false positives now ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Are we doing better with `class_weight=\"balanced\"`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dict = {}\n",
    "pipe_lr = make_pipeline(StandardScaler(), LogisticRegression(max_iter=500))\n",
    "scoring = [\"accuracy\", \"f1\", \"recall\", \"precision\", \"roc_auc\", \"average_precision\"]\n",
    "orig_scores = cross_validate(pipe_lr, X_train_big, y_train_big, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr_balanced = make_pipeline(\n",
    "    StandardScaler(), LogisticRegression(max_iter=500, class_weight=\"balanced\")\n",
    ")\n",
    "scoring = [\"accuracy\", \"f1\", \"recall\", \"precision\", \"roc_auc\", \"average_precision\"]\n",
    "bal_scores = cross_validate(pipe_lr_balanced, X_train_big, y_train_big, scoring=scoring)\n",
    "comp_dict = {\n",
    "    \"Original\": pd.DataFrame(orig_scores).mean().tolist(),\n",
    "    \"class_weight='balanced'\": pd.DataFrame(bal_scores).mean().tolist(),\n",
    "}\n",
    "pd.DataFrame(comp_dict, index=bal_scores.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall is much better but precision has dropped a lot; we have many false positives. \n",
    "- You could also optimize `class_weight` using hyperparameter optimization for your specific problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Changing the class weight will **generally reduce accuracy**.\n",
    "- The original model was trying to maximize accuracy.\n",
    "- Now you're telling it to do something different.\n",
    "- But that can be fine, accuracy isn't the only metric that matters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stratified Splits\n",
    "\n",
    "- A similar idea of \"balancing\" classes can be applied to data splits.\n",
    "- We have the same option in `train_test_split` with the `stratify` argument. \n",
    "- By default it splits the data so that if we have 10% negative examples in total, then each split will have 10% negative examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- If you are carrying out cross validation using `cross_validate`, by default it uses [`StratifiedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html). From the documentation: \n",
    "\n",
    "> This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.\n",
    "\n",
    "- In other words, if we have 10% negative examples in total, then each fold will have 10% negative examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Is stratifying a good idea? \n",
    "\n",
    "  - Well, it's no longer a random sample, which is probably theoretically bad, but not that big of a deal.\n",
    "  - If you have many examples, it shouldn't matter as much.\n",
    "  - It can be especially useful in multi-class, say if you have one class with very few cases.\n",
    "  - In general, these are difficult questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What did we learn today? \n",
    "\n",
    "- A number of possible ways to evaluate machine learning models \n",
    "    - Choose the evaluation metric that makes most sense in your context or which is most common in your discipline  \n",
    "- Two kinds of binary classification problems \n",
    "    - Distinguishing between two classes (e.g., dogs vs. cats)\n",
    "    - Spotting a class (e.g., spot fraud transaction, spot spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Precision, recall, f1-score are useful when dealing with spotting problems. \n",
    "- The thing that we are interested in spotting is considered \"positive\".   \n",
    "- Do you need to deal with class imbalance in the given problem? \n",
    "- Methods to deal with class imbalance \n",
    "    - Changing the training procedure \n",
    "        - `class_weight`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Relevant papers and resources \n",
    "\n",
    "- [The Relationship Between Precision-Recall and ROC Curves](https://www.biostat.wisc.edu/~page/rocpr.pdf)\n",
    "- [Article claiming that PR curve are better than ROC for imbalanced datasets](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118432)\n",
    "- [Precision-Recall-Gain Curves: PR Analysis Done Right](https://papers.nips.cc/paper/2015/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf)\n",
    "- [ROC animation](https://github.com/dariyasydykova/open_projects/tree/master/ROC_animation)\n",
    "- [Generalization in Adaptive Data Analysis and Holdout Reuse](https://arxiv.org/pdf/1506.02629.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:571]",
   "language": "python",
   "name": "conda-env-571-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
