{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e33c5b0c-d0f1-40c0-b794-3b3471ac73d2",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 8: Word embeddings, time series, and communication\n",
    "### Associated lectures: Lectures 16, 18, 19, and ML communication \n",
    "\n",
    "**Due date: December 07, 2021 at 11:59pm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e851ce52-f981-4c4e-9e9e-a740edd12e41",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Submission instructions](#sg) (4%)\n",
    "- [Exercise 1 - Exploring pre-trained word embeddings](#1) (24%)\n",
    "- [Exercise 2 - Exploring time series data](#2) (16%)\n",
    "- [Exercise 3 - Short answer questions](#4) (10%)\n",
    "- [Exercise 4 - Communication](#4) (46%)\n",
    "- (Optional)[Exercise 5 - Course take away](#5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4651888-484b-42a0-95e1-d273e5069205",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086914c2-5de1-414a-8770-23bef9f312d0",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe22cb5-f825-4dba-b5e3-3538f4afe703",
   "metadata": {},
   "source": [
    "## Instructions \n",
    "<hr>\n",
    "rubric={points:4}\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330/blob/master/docs/homework_instructions.md). \n",
    "\n",
    "**You may work on this homework in a group and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 3. \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be5b2d-1854-4c63-bcc6-9b6258b7293a",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b3f00-a3e5-45d8-b504-22a84ace38cd",
   "metadata": {},
   "source": [
    "## Exercise 1:  Exploring pre-trained word embeddings <a name=\"1\"></a>\n",
    "<hr>\n",
    "\n",
    "In lecture 16, we talked about natural language processing (NLP). Using pre-trained word embeddings is very common in NLP. It has been shown that pre-trained word embeddings [work well on a variety of text classification tasks](http://www.lrec-conf.org/proceedings/lrec2018/pdf/721.pdf). These embeddings are created by training a model like Word2Vec on a huge corpus of text such as a dump of Wikipedia or a dump of the web crawl. \n",
    "\n",
    "A number of pre-trained word embeddings are available out there. Some popular ones are: \n",
    "\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "    * trained using [the GloVe algorithm](https://nlp.stanford.edu/pubs/glove.pdf) \n",
    "    * published by Stanford University \n",
    "- [fastText pre-trained embeddings for 294 languages](https://fasttext.cc/docs/en/pretrained-vectors.html) \n",
    "    * trained using the fastText algorithm\n",
    "    * published by Facebook\n",
    "    \n",
    "In this exercise, you will be exploring GloVe Wikipedia pre-trained embeddings. The code below loads pre-trained word vectors trained on Wikipedia. (The vectors are created using an algorithm called GloVe.) To run the code, you'll need `gensim` package for that in your cpsc330 conda environment, which you can install as follows. \n",
    "\n",
    "```\n",
    "> conda activate cpsc330\n",
    "> conda install -c anaconda gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4823523-ca44-48a3-94bb-f6e453d27f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    "\n",
    "print(list(gensim.downloader.info()[\"models\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e4717e-215b-4c1b-b08a-9f5adbb52467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a while to run when you run it for the first time.\n",
    "import gensim.downloader as api\n",
    "\n",
    "glove_wiki_vectors = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ec38c4-ce89-4372-b015-035f4d682132",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(glove_wiki_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c78dafb-f712-447a-b870-1fac6c249e5f",
   "metadata": {},
   "source": [
    "There are 400,000 word vectors in these pre-trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a75ac-fd18-4a53-89d3-26f1051c4ef3",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119fb78-d2be-4ccf-8c8d-31026563e072",
   "metadata": {},
   "source": [
    "### 1.1 Word similarity using pre-trained embeddings\n",
    "rubric={points:4}\n",
    "\n",
    "Now that we have GloVe Wiki vectors (`glove_wiki_vectors`) loaded, let's explore the word vectors. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Calculate cosine similarity for the following word pairs (`word_pairs`) using the [`similarity`](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity) method of the model.\n",
    "2. Do the similarities make sense? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadd1aa6-6bb8-48d7-a959-691e19d411ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = [\n",
    "    (\"coast\", \"shore\"),\n",
    "    (\"clothes\", \"closet\"),\n",
    "    (\"old\", \"new\"),\n",
    "    (\"smart\", \"intelligent\"),\n",
    "    (\"dog\", \"cat\"),\n",
    "    (\"tree\", \"lawyer\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165ca2a9-eba8-472e-a49b-9d0561d96846",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0331ffe-bf58-4198-bd1e-3b62a5d34319",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d528120-c1ff-4203-82b8-404b62ba6bb0",
   "metadata": {},
   "source": [
    "### 1.2 Bias in embeddings\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "1. In Lecture 16 we saw that our pre-trained word embedding model output an analogy that reinforced a gender stereotype. Give an example of how using such a model could cause harm in the real world.\n",
    "2. Here we are using pre-trained embeddings which are built using Wikipedia data. Explore whether there are any worrisome biases present in these embeddings or not by trying out some examples. You can use the following two methods or other methods of your choice to explore what kind of stereotypes and biases are encoded in these embeddings. \n",
    "    - You can use the `analogy` function below which gives words analogies. \n",
    "    - You can also use [similarity](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity) or [distance](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=distance#gensim.models.keyedvectors.KeyedVectors.distances) methods. (An example is shown below.)   \n",
    "3. Discuss your observations. Do you observe the gender stereotype we observed in class in these embeddings?\n",
    "\n",
    "> Note that most of the recent embeddings are de-biased. But you might still observe some biases in them. Also, not all stereotypes present in pre-trained embeddings are necessarily bad. But you should be aware of them when you use them in your models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f7ef2b-d6b4-4338-a153-77fd4fc9847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(word1, word2, word3, model=glove_wiki_vectors):\n",
    "    \"\"\"\n",
    "    Returns analogy word using the given model.\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    word1 : (str)\n",
    "        word1 in the analogy relation\n",
    "    word2 : (str)\n",
    "        word2 in the analogy relation\n",
    "    word3 : (str)\n",
    "        word3 in the analogy relation\n",
    "    model :\n",
    "        word embedding model\n",
    "\n",
    "    Returns\n",
    "    ---------------\n",
    "        pd.dataframe\n",
    "    \"\"\"\n",
    "    print(\"%s : %s :: %s : ?\" % (word1, word2, word3))\n",
    "    sim_words = model.most_similar(positive=[word3, word2], negative=[word1])\n",
    "    return pd.DataFrame(sim_words, columns=[\"Analogy word\", \"Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d900c0-3027-4b9f-a750-bca946cf7bdb",
   "metadata": {},
   "source": [
    "An example of using similarity between words to explore biases and stereotypes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f0bacc-ba70-43e3-a7be-07c263f48048",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_wiki_vectors.similarity(\"white\", \"rich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2e6671-c6db-4cc9-9652-faba038e8e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_wiki_vectors.similarity(\"black\", \"rich\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbf431d-a56d-4d78-b381-eee21bceb049",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f04b87-5fa0-4eb4-bb50-cb21aa7ffd1c",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ccd3e-25c1-413d-b079-043fb2949090",
   "metadata": {},
   "source": [
    "### 1.3 Representation of all words in English\n",
    "rubric={reasoning:2}\n",
    "\n",
    "**Your tasks:**\n",
    "1. The vocabulary size of Wikipedia embeddings is quite large. Do you think it contains **all** words in English language? What would happen if you try to get a word vector that's unlikely to be present in the vocabulary (e.g., the word \"cpsc330\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d589eeae-c081-4042-8cdc-7dbc85a2c400",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464a0fbf-3a9c-42b3-b9cc-5dc474bc2804",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3cd04-30c9-43f4-9b07-6443ab4ecd7d",
   "metadata": {},
   "source": [
    "### 1.4 Classification with pre-trained embeddings\n",
    "rubric={points:8}\n",
    "\n",
    "In lecture 16, we saw that you can conveniently get word vectors with `spaCy` with `en_core_web_md` model. In this exercise, you'll use word embeddings in multi-class text classification task. We will use [HappyDB](https://www.kaggle.com/ritresearch/happydb) corpus which contains about 100,000 happy moments classified into 7 categories: *affection, exercise, bonding, nature, leisure, achievement, enjoy_the_moment*. The data was crowd-sourced via [Amazon Mechanical Turk](https://www.mturk.com/). The ground truth label is not available for all examples, and in this lab, we'll only use the examples where ground truth is available (~15,000 examples). \n",
    "\n",
    "- Download the data from [here](https://www.kaggle.com/ritresearch/happydb).\n",
    "- Unzip the file and copy it in the lab directory.\n",
    "\n",
    "We will be using spaCy in this exercise. If you do not have spaCy in your course environment, here is how you can install it.  \n",
    "\n",
    "```\n",
    "> conda activate cpsc330\n",
    "> conda install -c conda-forge spacy\n",
    "```\n",
    "\n",
    "- You also need to download the language model which contains all the pre-trained models. For that run the following in your course `conda` environment. \n",
    "```\n",
    "python -m spacy download en_core_web_md\n",
    "```\n",
    "\n",
    "The code below reads the data CSV (assuming that it's present in the current directory as *cleaned_hm.csv*),  cleans it up a bit, and splits it into train and test splits. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Train logistic regression with bag-of-words features and show classification report on the test set. \n",
    "2. Train logistic regression with average embedding representation extracted using spaCy and show classification report on the test set. (You can find an example of extracting average embedding features using spaCy in [lecture 16](https://ubc-cs.github.io/cpsc330/lectures/16_natural-language-processing.html#sentiment-classification-using-average-embeddings).)  \n",
    "3. Discuss your results. Which model is performing well. Which model would be more interpretable?  \n",
    "4. Are you observing any benefits of transfer learning here? Briefly discuss. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b35845-7976-4cda-b798-0c0700868fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cleaned_hm.csv\", index_col=0)\n",
    "sample_df = df.dropna()\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384fa13b-83a5-4e23-9280-c4e529937143",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yXKa7qfQXYPD",
    "outputId": "8bbf5eeb-0151-4853-a49c-3876279bbeb7"
   },
   "outputs": [],
   "source": [
    "sample_df = sample_df.rename(\n",
    "    columns={\"cleaned_hm\": \"moment\", \"ground_truth_category\": \"target\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de04d594-7174-409d-a9fa-c2fd738e2208",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(sample_df, test_size=0.3, random_state=123)\n",
    "X_train, y_train = train_df[\"moment\"], train_df[\"target\"]\n",
    "X_test, y_test = test_df[\"moment\"], test_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f63a73-0d13-4276-9e79-7ad60575a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f345014-d6cc-46e6-b964-4dd412f46a04",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a6ab6-62ef-4fdd-ba0d-5e7e920154a3",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec620e19-016a-4476-bb7e-de0c402078d2",
   "metadata": {},
   "source": [
    "## Exercise 2: Exploring time series data <a name=\"2\"></a>\n",
    "<hr>\n",
    "\n",
    "In this exercise we'll be looking at a [dataset of avocado prices](https://www.kaggle.com/neuromusic/avocado-prices). You should start by downloading the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22471fe-942e-49aa-8fb8-9d4fbf6b00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"avocado.csv\", parse_dates=[\"Date\"], index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3425e59d-9580-4512-8bd2-c8c9b836df8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0aa8d9-34b6-4401-8b91-77e1342510ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Date\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b64f1d1-9614-44df-b625-52a77c00af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Date\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dae5238-ac94-4b1b-b368-d972b6582d8a",
   "metadata": {},
   "source": [
    "It looks like the data ranges from the start of 2015 to March 2018 (~3 years ago), for a total of 3.25 years or so. Let's split the data so that we have a 6 months of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4d6fb5-1cbb-47e0-a5c4-34b66d026d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = \"20170925\"\n",
    "train_df = df[df[\"Date\"] <= split_date]\n",
    "test_df = df[df[\"Date\"] > split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a3b6ae-1406-48c3-8b5c-e7b65a041852",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_df) + len(test_df) == len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ed401-224b-42d3-a4bd-f67b60c3625b",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "rubric={points:4}\n",
    "\n",
    "In the Rain is Australia dataset from lecture, we had different measurements for each Location. What about this dataset: for which categorical feature(s), if any, do we have separate measurements? Justify your answer by referencing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8d3fbd-8c98-4e3f-8ca5-22571c563d1f",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17a187-bf66-4339-b5f4-3f79cb6948cc",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b56b13-e2ff-45d9-b800-fb7006f92653",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "rubric={points:4}\n",
    "\n",
    "In the Rain in Australia dataset, the measurements were generally equally spaced but with some exceptions. How about with this dataset? Justify your answer by referencing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f48e7c-1394-42d7-b481-e910c9ad8986",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dc1348-c2c6-46f6-bbdf-a77810930ac1",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ab120c-6b2b-4dfb-8765-7367a9577482",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "rubric={points:4}\n",
    "\n",
    "In the Rain is Australia dataset, each location was a different place in Australia. For this dataset, look at the names of the regions. Do you think the regions are all distinct, or are there overlapping regions? Justify your answer by referencing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c557cb1f-155f-4eb2-99fe-b93473ba15fd",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331ec42a-3093-46c5-b708-ca7716f939dc",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb32828-c8f6-4344-90bc-7ec214e448e7",
   "metadata": {},
   "source": [
    "We will use the entire dataset despite any location-based weirdness uncovered in the previous part.\n",
    "\n",
    "We would like to forecast the avocado price, which is the `AveragePrice` column. The function below is adapted from Lecture 18, with some improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef9e53-9170-4a0c-a249-7cf0715496b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_feature(\n",
    "    df, orig_feature, lag, groupby, new_feature_name=None, clip=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a new feature that's a lagged version of an existing one.\n",
    "\n",
    "    NOTE: assumes df is already sorted by the time columns and has unique indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.core.frame.DataFrame\n",
    "        The dataset.\n",
    "    orig_feature : str\n",
    "        The column name of the feature we're copying\n",
    "    lag : int\n",
    "        The lag; negative lag means values from the past, positive lag means values from the future\n",
    "    groupby : list\n",
    "        Column(s) to group by in case df contains multiple time series\n",
    "    new_feature_name : str\n",
    "        Override the default name of the newly created column\n",
    "    clip : bool\n",
    "        If True, remove rows with a NaN values for the new feature\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.core.frame.DataFrame\n",
    "        A new dataframe with the additional column added.\n",
    "\n",
    "    TODO: could/should simplify this function by using `df.shift()`\n",
    "    \"\"\"\n",
    "\n",
    "    if new_feature_name is None:\n",
    "        if lag < 0:\n",
    "            new_feature_name = \"%s_lag%d\" % (orig_feature, -lag)\n",
    "        else:\n",
    "            new_feature_name = \"%s_ahead%d\" % (orig_feature, lag)\n",
    "\n",
    "    new_df = df.assign(**{new_feature_name: np.nan})\n",
    "    for name, group in new_df.groupby(groupby):\n",
    "        if lag < 0:  # take values from the past\n",
    "            new_df.loc[group.index[-lag:], new_feature_name] = group.iloc[:lag][\n",
    "                orig_feature\n",
    "            ].values\n",
    "        else:  # take values from the future\n",
    "            new_df.loc[group.index[:-lag], new_feature_name] = group.iloc[lag:][\n",
    "                orig_feature\n",
    "            ].values\n",
    "\n",
    "    if clip:\n",
    "        new_df = new_df.dropna(subset=[new_feature_name])\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cbe62a-05a6-4bed-8ae9-b1bb7299c16a",
   "metadata": {},
   "source": [
    "We first sort our dataframe properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0049d46-d7e2-40a4-9d41-74ce2e875a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sort = df.sort_values(by=[\"region\", \"type\", \"Date\"]).reset_index(drop=True)\n",
    "df_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaee71e-d45c-48dc-81a3-ebf7195cbea4",
   "metadata": {},
   "source": [
    "We then call `create_lag_feature`. This creates a new column in the dataset `AveragePriceNextWeek`, which is the following week's `AveragePrice`. We have set `clip=True` which means it will remove rows where the target would be missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90736df7-04b7-40d4-a835-e8eb899da7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hastarget = create_lag_feature(\n",
    "    df_sort, \"AveragePrice\", +1, [\"region\", \"type\"], \"AveragePriceNextWeek\", clip=True\n",
    ")\n",
    "df_hastarget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbfa73e-48d7-49be-af1c-dba397d9f946",
   "metadata": {},
   "source": [
    "I will now split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e952769-dbd1-4052-855f-2d2f2b4101d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_hastarget[df_hastarget[\"Date\"] <= split_date]\n",
    "test_df = df_hastarget[df_hastarget[\"Date\"] > split_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fb19b2-0a17-4133-bf78-3dea9aa61526",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848e951f-8dde-4a34-bf4e-b2c3a6270bb0",
   "metadata": {},
   "source": [
    "### 2.4 Baseline\n",
    "rubric={points:4}\n",
    "\n",
    "Let's try a baseline. Previously we used `DummyClassifier` or `DummyRegressor` as a baseline. This time, we'll do something else as a baseline: we'll assume the price stays the same from this week to next week. So, we'll set our prediction of \"AveragePriceNextWeek\" exactly equal to \"AveragePrice\", assuming no change. That is kind of like saying, \"If it's raining today then I'm guessing it will be raining tomorrow\". This simplistic approach will not get a great score but it's a good starting point for reference. If our model does worse that this, it must not be very good. \n",
    "\n",
    "Using this baseline approach, what $R^2$ do you get?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8369f38-926b-4428-a276-dc1719b94d50",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d241cd-4669-46d9-bb21-1fd8006a39e8",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0615dc-63c2-458f-844b-f17d30758ab1",
   "metadata": {},
   "source": [
    "### (Optional) 2.5 Modeling\n",
    "rubric={points:2}\n",
    "\n",
    "Now that the baseline is done, let's build some models to forecast the average avocado price a week later. Experiment with a few approachs for encoding the date. Justify the decisions you make. Which approach worked best? Report your test score and briefly discuss your results.\n",
    "\n",
    "> Because we only have 2 splits here, we need to be a bit wary of overfitting on the test set. Try not to test on it a ridiculous number of times. If you are interested in some proper ways of dealing with this, see for example sklearn's [TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html), which is like cross-validation for time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fe2285-38d1-409a-9f4c-3f5dfd50622a",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ad9e61-e5f6-4025-8356-f48f651a8e1e",
   "metadata": {},
   "source": [
    "## Exercise 3: Short answer questions <a name=\"3\"></a>\n",
    "\n",
    "Each question is worth 2 points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972060d9-742d-47ae-82c5-74b258de16e7",
   "metadata": {},
   "source": [
    "### 3.1\n",
    "rubric={points:4}\n",
    "\n",
    "The following questions pertain to Lecture 18 on time series data:\n",
    "\n",
    "1. Sometimes a time series has missing time points or, worse, time points that are unequally spaced in general. Give an example of a real world situation where the time series data would have unequally spaced time points.\n",
    "2. In class we discussed two approaches to using temporal information: encoding the date as one or more features, and creating lagged versions of features. Which of these (one/other/both/neither) two approaches would struggle with unequally spaced time points? Briefly justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfb63fb-80d3-4cd4-a1f0-38ed8bfe3a21",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef053d93-f20e-417f-81ae-35edb701020c",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3703ae-7d56-4987-b70b-4e222f1d8b15",
   "metadata": {},
   "source": [
    "### 3.2\n",
    "rubric={points:6}\n",
    "\n",
    "The following questions pertain to Lecture 19 on survival analysis. We'll consider the use case of customer churn analysis.\n",
    "\n",
    "1. What is the problem with simply labeling customers are \"churned\" or \"not churned\" and using standard supervised learning techniques, as we did in hw5?\n",
    "2. Consider customer A who just joined last week vs. customer B who has been with the service for a year. Who do you expect will leave the service first: probably customer A, probably customer B, or we don't have enough information to answer?\n",
    "3. If a customer's survival function is almost flat during a certain period, how do we interpret that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8507d-3e72-469b-a946-05c816be18a4",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8ffdd0-8c1e-4042-8414-ac8e57caf980",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed594c68-91c3-45d9-baec-6ac38a02c971",
   "metadata": {},
   "source": [
    "## Exercise 4: Communication <a name=\"4\"></a>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5767926d-c17d-4a93-b59a-7242a4c76ff0",
   "metadata": {},
   "source": [
    "### 4.1 Blog post \n",
    "rubric={points:40}\n",
    "\n",
    "Write up your analysis from hw6 or any other assignment or your side project on machine learning in a \"blog post\" or report format. It's fine if you just write it here in this notebook. Alternatively, you can publish your blog post publicly and include a link here. (See exercise 4.3.) The target audience for your blog post is someone like yourself right before you took this course. They don't necessarily have ML knowledge, but they have a solid foundation in technical matters. The post should focus on explaining **your results and what you did** in a way that's understandable to such a person, **not** a lesson trying to teach someone about machine learning. Again: focus on the results and why they are interesting; avoid pedagogical content.\n",
    "\n",
    "Your post must include the following elements (not necessarily in this order):\n",
    "\n",
    "- Description of the problem/decision.\n",
    "- Description of the dataset (the raw data and/or some EDA).\n",
    "- Description of the model.\n",
    "- Description your results, both quantitatively and qualitatively. Make sure to refer to the original problem/decision.\n",
    "- A section on caveats, describing at least 3 reasons why your results might be incorrect, misleading, overconfident, or otherwise problematic. Make reference to your specific dataset, model, approach, etc. To check that your reasons are specific enough, make sure they would not make sense, if left unchanged, to most students' submissions; for example, do not just say \"overfitting\" without explaining why you might be worried about overfitting in your specific case.\n",
    "- At least 3 visualizations. These visualizations must be embedded/interwoven into the text, not pasted at the end. The text must refer directly to each visualization. For example \"as shown below\" or \"the figure demonstrates\" or \"take a look at Figure 1\", etc. It is **not** sufficient to put a visualization in without referring to it directly.\n",
    "\n",
    "A reasonable length for your entire post would be **800 words**. The maximum allowed is **1000 words**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6169eefb-18a8-4e13-b7ca-1b3539a79215",
   "metadata": {},
   "source": [
    "#### Example blog posts\n",
    "\n",
    "Here are some examples of applied ML blog posts that you may find useful as inspiration. The target audiences of these posts aren't necessarily the same as yours, and these posts are longer than yours, but they are well-structured and engaging. You are **not required to read these** posts as part of this assignment - they are here only as examples if you'd find that useful.\n",
    "\n",
    "From the UBC Master of Data Science blog, written by a past student:\n",
    "\n",
    "- https://ubc-mds.github.io/2019-07-26-predicting-customer-probabilities/\n",
    "\n",
    "This next one uses R instead of Python, but that might be good in a way, as you can see what it's like for a reader that doesn't understand the code itself (the target audience for your post here):\n",
    "\n",
    "- https://rpubs.com/RosieB/taylorswiftlyricanalysis\n",
    "\n",
    "Finally, here are a couple interviews with winners from Kaggle competitions. The format isn't quite the same as a blog post, but you might find them interesting/relevant:\n",
    "\n",
    "- https://medium.com/kaggle-blog/instacart-market-basket-analysis-feda2700cded\n",
    "- https://medium.com/kaggle-blog/winner-interview-with-shivam-bansal-data-science-for-good-challenge-city-of-los-angeles-3294c0ed1fb2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfdd094-eeb6-4f00-a3bc-5a6105eedb12",
   "metadata": {},
   "source": [
    "#### A note on plagiarism\n",
    "\n",
    "You may **NOT** include text or visualizations that were not written/created by you. If you are in any doubt as to what constitutes plagiarism, please just ask. For more information see the [UBC Academic Misconduct policies](http://www.calendar.ubc.ca/vancouver/index.cfm?tree=3,54,111,959). Please don't copy this from somewhere üôè. If you can't do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4052395d-a695-4063-97b6-46c4e13016d8",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59667c9-db6a-4c12-a556-5b9815ef3564",
   "metadata": {},
   "source": [
    "### 4.2\n",
    "rubric={points:6}\n",
    "\n",
    "Describe one effective communication technique that you used in your post, or an aspect of the post that you are particularly satisfied with.\n",
    "\n",
    "Max 3 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea9c37-34c9-4b3e-a2df-e00cedd3e8ae",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56965b8c-9f4d-4a68-be4c-2a745dcf9989",
   "metadata": {},
   "source": [
    "## (optional, not for marks) 4.3\n",
    "\n",
    "Publish your blog post from 4.1 publicly using a tool like Hugo, or somewhere like medium.com, and paste a link here. Be sure to pick a tool in which code and code output look reasonable. This link could be a useful line on your resume!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a55e63-38cb-4e36-867f-0261d1c583de",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cefc8e-cf76-4c27-9aa6-c560cbc0fc2b",
   "metadata": {},
   "source": [
    "### (Optional) Exercise 5 <a name=\"5\"></a>\n",
    "rubric={points:1}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "What is your biggest takeaway from this course? \n",
    "\n",
    "> I'm looking forward to read your answers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fb9e2f-a2d2-4f56-9fb4-c91492e4801b",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab723dc5-4ea6-4c44-ace9-bf345bf8c120",
   "metadata": {},
   "source": [
    "## Submission instructions \n",
    "\n",
    "**PLEASE READ:** When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from ‚Äú1‚Äù will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4e160c-d947-4123-8e67-fa3c89c9aa8f",
   "metadata": {},
   "source": [
    "### Congratulations on finishing all homework assignments! :clap: :clap: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3bee4-0171-4465-838f-e5ac8a943e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(\"eva-congrats.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
