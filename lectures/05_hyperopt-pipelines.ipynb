{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CPSC 330 Lecture 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lecture plan\n",
    "\n",
    "- Announcements\n",
    "- Hyperparameter optimization (25 min)\n",
    "- Break (5 min)\n",
    "- Pipelines intro (10 min)\n",
    "- Pipelines and the Golden Rule (20 min)\n",
    "- Pipelines and hyperparameter tuning (5 min)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "add designated question periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from sklearn.feature_selection import RFE, RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Announcements\n",
    "\n",
    "- Midterm grades returned.\n",
    "- hw5 released last week, deadline extended to **Friday** at 6pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperparameter optimization (25 min)\n",
    "\n",
    "#### Manual hyperparameter optimization\n",
    "\n",
    "- We tried this last class.\n",
    "- Advantage: we may have some intuition about what might work.\n",
    "- Disadvantage: it takes a lot of work.\n",
    "- Disadvantage: in very complicated cases, our intuition might be worse than a data-driven approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Automated hyperparameter optimization\n",
    "\n",
    "- Advantage: reduce human effort\n",
    "- Advantage: less prone to error and improve reproducibility\n",
    "- Advantage: data-driven approaches may be effective\n",
    "- Disadvantage: may be hard to incorporate intuition\n",
    "- Disadvantage: be careful about overfitting on the validation set.\n",
    "\n",
    "\n",
    "\n",
    "There are two automated hyperparameter search methods in scikit-learn:\n",
    "\n",
    "  - Exhaustive grid search: [`sklearn.model_selection.GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "  - Randomized hyperparameter optimization: [`sklearn.model_selection.RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exhaustive grid search\n",
    "\n",
    "- A user specifies a set of values for each hyperparameter. \n",
    "- The method considers \"product\" of the sets and then evaluates each combination one by one.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remind ourselves how things were going last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(rf, X_train_transformed, y_train, cv=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8541925480371526"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the automated hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': [10, 100], 'max_depth': [3, None], 'max_features': [3, None]}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "              \"n_estimators\"     : [10,100],\n",
    "              \"max_depth\"        : [3, None],\n",
    "              \"max_features\"     : [3, None]\n",
    "             }\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How many combinations in total? \n",
    "- $2\\times 2\\times 2=8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(list(map(len, param_grid.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=321)\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:  1.3min finished\n"
     ]
    }
   ],
   "source": [
    "grid_search.fit(X_train_transformed, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': None, 'max_features': None, 'n_estimators': 100}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lol... these are the default values.\n",
    "- I guess they picked good defaults!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8549216369281329"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.854922</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>14.519656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.848587</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>1.399292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.848127</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>1.986127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.844748</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.517570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.844748</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>5.280527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.839527</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.226066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.761057</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.189320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.760519</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.907236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean_test_score param_max_depth param_max_features  \\\n",
       "rank_test_score                                                       \n",
       "1                       0.854922            None               None   \n",
       "2                       0.848587            None               None   \n",
       "3                       0.848127            None                  3   \n",
       "4                       0.844748               3               None   \n",
       "4                       0.844748               3               None   \n",
       "6                       0.839527            None                  3   \n",
       "7                       0.761057               3                  3   \n",
       "8                       0.760519               3                  3   \n",
       "\n",
       "                param_n_estimators  mean_fit_time  \n",
       "rank_test_score                                    \n",
       "1                              100      14.519656  \n",
       "2                               10       1.399292  \n",
       "3                              100       1.986127  \n",
       "4                               10       0.517570  \n",
       "4                              100       5.280527  \n",
       "6                               10       0.226066  \n",
       "7                               10       0.189320  \n",
       "8                              100       0.907236  "
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_search.cv_results_)[['mean_test_score', 'param_max_depth', 'param_max_features', 'param_n_estimators', 'mean_fit_time', 'rank_test_score']].set_index(\"rank_test_score\").sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that the grid search object acts like a scikit-learn model.\n",
    "- It was actually refit on the _whole_ training set, as discussed earlier in the course!\n",
    "- I believe it is the same as `grid_search.best_estimator_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<=50K', '>50K', '<=50K', ..., '>50K', '<=50K', '<=50K'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Problems with exhaustive grid search \n",
    "\n",
    "- Required number of models to evaluate grows exponentially with the dimensionally of the configuration space. \n",
    "- Exhaustive search may become infeasible fairly quickly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Randomized hyperparameter search\n",
    "\n",
    "- Randomized hyperparameter optimization: [`sklearn.model_selection.RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "- Samples configurations at random until certain budget (e.g., time) is exhausted.\n",
    "- Advantage: you can choose how many runs you'll do.\n",
    "- Advantage: you can restrict yourself less on what values you might try.\n",
    "- Advantage: Adding parameters that do not influence the performance does not affect efficiency.\n",
    "- Advantage: research shows this is generally a better idea than grid search, see image for intuition:\n",
    "\n",
    "![](img/randomsearch_bergstra.png)\n",
    "\n",
    "Source: [Bergstra and Bengio, Random Search for Hyper-Parameter Optimization, JMLR 2012](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf).\n",
    "\n",
    "- You don't know in advance which hyperparameters are important for your problem.\n",
    "- But some of them might be unimportant.\n",
    "- In the left figure, 6 of the 9 searches are useless because they are only varying the unimportant parameter.\n",
    "- In the right figure, all 9 searches are useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to syntax. We can have the parameters chosen from a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_choices = {\n",
    "              \"n_estimators\"     : [10, 30, 100, 300],\n",
    "              \"max_depth\"        : [3, 10, None],\n",
    "              \"max_features\"     : [3, 10, None]\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also give it distributions, instead of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "              \"n_estimators\"     : scipy.stats.randint(low=10, high=300),\n",
    "              \"max_depth\"        : scipy.stats.randint(low=10, high=30),\n",
    "              \"max_features\"     : scipy.stats.randint(low=10, high=30)\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=321) # Note: you can set other hyperparameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(rf, param_distributions = param_dist, \n",
    "                                   n_iter = 10, \n",
    "                                   cv=3,\n",
    "                                   verbose=1, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "random_search.fit(X_train_transformed, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: some hyperparameters significantly affect the training time!\n",
    "- For example, setting `n_estimators=1000` is going to be very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 16, 'max_features': 27, 'n_estimators': 93}"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we get something different! \n",
    "- What's the score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.863175750441226"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, we had 85.4% and now we have 86.1%.\n",
    "- Is that difference important?\n",
    "- Do we BELIEVE that difference?\n",
    "  - We can try it out on the test set.\n",
    "- But first:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.863176</td>\n",
       "      <td>16</td>\n",
       "      <td>27</td>\n",
       "      <td>93</td>\n",
       "      <td>4.662635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.862561</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>106</td>\n",
       "      <td>2.776636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.861640</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>108</td>\n",
       "      <td>3.506636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.861602</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>94</td>\n",
       "      <td>2.629149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.861333</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>218</td>\n",
       "      <td>5.281985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.860527</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>83</td>\n",
       "      <td>3.811981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.860450</td>\n",
       "      <td>25</td>\n",
       "      <td>29</td>\n",
       "      <td>263</td>\n",
       "      <td>15.884677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.860412</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>234</td>\n",
       "      <td>8.109985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.859951</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>145</td>\n",
       "      <td>7.407122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.859375</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>0.542452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean_test_score param_max_depth param_max_features  \\\n",
       "rank_test_score                                                       \n",
       "1                       0.863176              16                 27   \n",
       "2                       0.862561              20                 11   \n",
       "3                       0.861640              23                 12   \n",
       "4                       0.861602              17                 12   \n",
       "5                       0.861333              14                 10   \n",
       "6                       0.860527              27                 25   \n",
       "7                       0.860450              25                 29   \n",
       "8                       0.860412              10                 24   \n",
       "9                       0.859951              25                 26   \n",
       "10                      0.859375              14                 27   \n",
       "\n",
       "                param_n_estimators  mean_fit_time  \n",
       "rank_test_score                                    \n",
       "1                               93       4.662635  \n",
       "2                              106       2.776636  \n",
       "3                              108       3.506636  \n",
       "4                               94       2.629149  \n",
       "5                              218       5.281985  \n",
       "6                               83       3.811981  \n",
       "7                              263      15.884677  \n",
       "8                              234       8.109985  \n",
       "9                              145       7.407122  \n",
       "10                              12       0.542452  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(random_search.cv_results_)[['mean_test_score', 'param_max_depth', 'param_max_features', 'param_n_estimators', 'mean_fit_time', 'rank_test_score']].set_index(\"rank_test_score\").sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Look at the timings, they are quite interesting.\n",
    "- And now, the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8527560264087211"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8622754491017964"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fancier methods\n",
    "\n",
    "- Both `GridSearchCV` and `RandomizedSearchCV` do each trial independently.\n",
    "- What if you could learn from your experience, e.g. learn that `max_depth=3` is bad?\n",
    "  - That could save time because you wouldn't try combinations involving `max_depth=3` in the future.\n",
    "- We can do this with `scikit-optimize`, which is a completely different package from `scikit-learn`\n",
    "- It uses a technique called \"model-based optimization\" and we'll specifically use \"Bayesian optimization\".\n",
    "  - In short, it uses machine learning to predict what hyperparameters will be good.\n",
    "  - Machine learning on machine learning!\n",
    "- As it happens I did my PhD thesis on this topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `BayesSearchCV` uses the same interface as `GridSearchCV` and `RandomSearchCV`.\n",
    "- However, the way we specify the parameter distributions is slightly different.\n",
    "- Here, we can just give the bounds as tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_opt = BayesSearchCV(\n",
    "    RandomForestClassifier(random_state=321),\n",
    "    {\n",
    "        'n_estimators': (10, 300),  \n",
    "        'max_depth': (3, 30),\n",
    "        'max_features': (3, 30)\n",
    "    },\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    random_state=123,\n",
    "    verbose=0,\n",
    "    refit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 41s, sys: 3.35 s, total: 2min 44s\n",
      "Wall time: 3min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=3, error_score='raise',\n",
       "              estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                               class_weight=None,\n",
       "                                               criterion='gini', max_depth=None,\n",
       "                                               max_features='auto',\n",
       "                                               max_leaf_nodes=None,\n",
       "                                               max_samples=None,\n",
       "                                               min_impurity_decrease=0.0,\n",
       "                                               min_impurity_split=None,\n",
       "                                               min_samples_leaf=1,\n",
       "                                               min_samples_split=2,\n",
       "                                               min_weight_fraction_leaf=0.0,\n",
       "                                               n_estimators=100, n_jobs=None,\n",
       "                                               oob_score=False,\n",
       "                                               random_state=321, verbose=0,\n",
       "                                               warm_start=False),\n",
       "              fit_params=None, iid=True, n_iter=10, n_jobs=1, n_points=1,\n",
       "              optimizer_kwargs=None, pre_dispatch='2*n_jobs', random_state=123,\n",
       "              refit=True, return_train_score=False, scoring=None,\n",
       "              search_spaces={'max_depth': (3, 30), 'max_features': (3, 30),\n",
       "                             'n_estimators': (10, 300)},\n",
       "              verbose=0)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "bayes_opt.fit(X_train_transformed, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It took a similar amount of time to the other methods.\n",
    "- In reality there is some extra computation to do the \"meta-ML\".\n",
    "- However, the overall time is dominated by the time of calling `fit` on the random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 18, 'max_features': 12, 'n_estimators': 130}"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_opt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8625998157248157"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_opt.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The score looks promising.\n",
    "- In theory, it should get even better as we increase `n_iter` (because it has more data to learn from).\n",
    "- Checking the test score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8621219100261016"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_opt.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And reproducing the previous test scores for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8622754491017964"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8527560264087211"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this case, it seems we weren't overfitting on the validation set and this exercise was actually useful.\n",
    "- Should I always use this? Not necessarily.\n",
    "- Disadvantage: requires installation.\n",
    "- Disadvantage: when number of trials is large (e.g. hundreds), the meta-ML can actually get too slow.\n",
    "- Disadvantage: harder parallelize the search because each trial depends on the previous ones.\n",
    "  - Note `n_jobs` parameter for `GridSearchCV` and `RandomizedSearchCV`.  \n",
    "  - `BayesSearchCV` also has this parameter.\n",
    "  - It can definitely parallelize the folds.\n",
    "  - The search will be less effective if it parallelizes further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can I generalize this to say `BayesSearchCV` > `RandomizedSearchCV` > `GridSearchCV`?\n",
    "- Not quite. I'd say `RandomizedSearchCV` > `GridSearchCV` is pretty reasonable\n",
    "- But we should think a bit more carefully about `BayesSearchCV` for the above reasons.\n",
    "- `RandomizedSearchCV` is often a reasonable choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Break (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines intro (10 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to our house price prediction dataset from last week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/week_05_house-prices/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trainvalid, df_test = train_test_split(df, test_size=0.1, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid = train_test_split(df_trainvalid, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target               = ['SalePrice']\n",
    "drop_features        = ['Id']\n",
    "numeric_features     = ['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', \n",
    "                        'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', \n",
    "                        'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', \n",
    "                        'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', \n",
    "                        'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', \n",
    "                        'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', \n",
    "                        'ScreenPorch', 'PoolArea', 'MiscVal', 'YrSold']\n",
    "ordinal_features_reg = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', \n",
    "                        'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']\n",
    "ordinal_features_oth = ['BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', \n",
    "                        'Functional',  'Fence']\n",
    "categorical_features = list(set(df_train.columns) - set(target) - set(drop_features) - \n",
    "                            set(numeric_features) - \n",
    "                            set(ordinal_features_reg) - set(ordinal_features_oth))\n",
    "all_features = numeric_features + ordinal_features_reg + categorical_features + ordinal_features_oth\n",
    "\n",
    "ordering = ['Po', 'Fa', 'TA', 'Gd', 'Ex'] # if N/A it will just impute something, per below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train[\"SalePrice\"]\n",
    "y_valid = df_valid[\"SalePrice\"]\n",
    "y_trainvalid = df_trainvalid[\"SalePrice\"]\n",
    "y_test  = df_test[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_log = np.log(y_train)\n",
    "y_valid_log = np.log(y_valid)\n",
    "y_trainvalid_log = np.log(y_trainvalid)\n",
    "y_test_log  = np.log(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the next steps?\n",
    "\n",
    "- Feature transformations/encodings.\n",
    "- What if we have many?\n",
    "- These can be combined into a scikit-learn [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal', OrdinalEncoder(categories=[ordering for i in ordinal_features_reg]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='?')),\n",
    "    ('onehot', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ColumnTransformer` now combines these `Pipeline` objects instead of the individual transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    ('numeric', numeric_transformer, numeric_features),\n",
    "    ('ordinal', ordinal_transformer, ordinal_features_reg),\n",
    "    ('categorical', categorical_transformer, categorical_features + ordinal_features_oth)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next line fits all 6 transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit(df_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = preprocessor.named_transformers_['categorical'].named_steps['onehot']\n",
    "ohe_feature_names = list(ohe.get_feature_names(categorical_features + ordinal_features_oth))\n",
    "new_columns = numeric_features + ordinal_features_reg + ohe_feature_names\n",
    "new_columns;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc = pd.DataFrame(preprocessor.transform(df_train), index=df_train.index, columns=new_columns)\n",
    "X_valid_enc = pd.DataFrame(preprocessor.transform(df_valid), index=df_valid.index, columns=new_columns)\n",
    "X_test_enc  = pd.DataFrame(preprocessor.transform(df_test),  index=df_test.index,  columns=new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>...</th>\n",
       "      <th>Functional_Maj2</th>\n",
       "      <th>Functional_Min1</th>\n",
       "      <th>Functional_Min2</th>\n",
       "      <th>Functional_Mod</th>\n",
       "      <th>Functional_Typ</th>\n",
       "      <th>Fence_?</th>\n",
       "      <th>Fence_GdPrv</th>\n",
       "      <th>Fence_GdWo</th>\n",
       "      <th>Fence_MnPrv</th>\n",
       "      <th>Fence_MnWw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>-0.165978</td>\n",
       "      <td>0.338699</td>\n",
       "      <td>-0.062802</td>\n",
       "      <td>-0.505587</td>\n",
       "      <td>1.048406</td>\n",
       "      <td>0.927764</td>\n",
       "      <td>-0.577947</td>\n",
       "      <td>0.791412</td>\n",
       "      <td>-0.284437</td>\n",
       "      <td>-0.622612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>-0.026237</td>\n",
       "      <td>-0.354633</td>\n",
       "      <td>-0.764983</td>\n",
       "      <td>-1.415471</td>\n",
       "      <td>-0.731683</td>\n",
       "      <td>-1.679272</td>\n",
       "      <td>0.294327</td>\n",
       "      <td>-0.544487</td>\n",
       "      <td>-0.284437</td>\n",
       "      <td>-0.120404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>1.464330</td>\n",
       "      <td>-0.133037</td>\n",
       "      <td>-0.062802</td>\n",
       "      <td>-0.505587</td>\n",
       "      <td>-0.328644</td>\n",
       "      <td>-1.099931</td>\n",
       "      <td>1.074497</td>\n",
       "      <td>-0.149451</td>\n",
       "      <td>-0.284437</td>\n",
       "      <td>0.336963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>-0.259139</td>\n",
       "      <td>-0.354322</td>\n",
       "      <td>-0.764983</td>\n",
       "      <td>-0.505587</td>\n",
       "      <td>0.242328</td>\n",
       "      <td>-0.279197</td>\n",
       "      <td>-0.577947</td>\n",
       "      <td>-0.937398</td>\n",
       "      <td>-0.284437</td>\n",
       "      <td>-1.295212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>-0.026237</td>\n",
       "      <td>0.084927</td>\n",
       "      <td>-0.062802</td>\n",
       "      <td>0.404297</td>\n",
       "      <td>0.175155</td>\n",
       "      <td>-0.375754</td>\n",
       "      <td>-0.577947</td>\n",
       "      <td>0.691592</td>\n",
       "      <td>-0.284437</td>\n",
       "      <td>-0.306490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 288 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LotFrontage   LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "1341    -0.165978  0.338699    -0.062802    -0.505587   1.048406   \n",
       "459     -0.026237 -0.354633    -0.764983    -1.415471  -0.731683   \n",
       "367      1.464330 -0.133037    -0.062802    -0.505587  -0.328644   \n",
       "894     -0.259139 -0.354322    -0.764983    -0.505587   0.242328   \n",
       "672     -0.026237  0.084927    -0.062802     0.404297   0.175155   \n",
       "\n",
       "      YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  BsmtUnfSF  ...  \\\n",
       "1341      0.927764   -0.577947    0.791412   -0.284437  -0.622612  ...   \n",
       "459      -1.679272    0.294327   -0.544487   -0.284437  -0.120404  ...   \n",
       "367      -1.099931    1.074497   -0.149451   -0.284437   0.336963  ...   \n",
       "894      -0.279197   -0.577947   -0.937398   -0.284437  -1.295212  ...   \n",
       "672      -0.375754   -0.577947    0.691592   -0.284437  -0.306490  ...   \n",
       "\n",
       "      Functional_Maj2  Functional_Min1  Functional_Min2  Functional_Mod  \\\n",
       "1341              0.0              0.0              0.0             0.0   \n",
       "459               0.0              0.0              0.0             0.0   \n",
       "367               0.0              0.0              0.0             0.0   \n",
       "894               0.0              0.0              0.0             0.0   \n",
       "672               0.0              0.0              0.0             0.0   \n",
       "\n",
       "      Functional_Typ  Fence_?  Fence_GdPrv  Fence_GdWo  Fence_MnPrv  \\\n",
       "1341             1.0      1.0          0.0         0.0          0.0   \n",
       "459              1.0      1.0          0.0         0.0          0.0   \n",
       "367              1.0      1.0          0.0         0.0          0.0   \n",
       "894              1.0      1.0          0.0         0.0          0.0   \n",
       "672              1.0      1.0          0.0         0.0          0.0   \n",
       "\n",
       "      Fence_MnWw  \n",
       "1341         0.0  \n",
       "459          0.0  \n",
       "367          0.0  \n",
       "894          0.0  \n",
       "672          0.0  \n",
       "\n",
       "[5 rows x 288 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So far, we just accomplished the same thing with cleaner code.\n",
    "  - Helps avoid messing up the order of the features.\n",
    "- Next, we will use this to accomplish something new.\n",
    "- But first..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines and the Golden Rule (20 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did something like this in the hw4 solution (note I'm just using the `model` here, not the `pipeline`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainvalid_enc = pd.concat((X_train_enc, X_valid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05559473, -0.04102244, -0.05763048, -0.0949194 , -0.07345046,\n",
       "       -0.06285405, -0.05423519, -0.02718618, -0.01933039, -0.06156132])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(model, X_trainvalid_enc, y_trainvalid_log, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the problem here?\n",
    "  - Discuss for 2-3 minutes.\n",
    "  - Don't look ahead!\n",
    "\n",
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The folds are a random reshuffling.\n",
    "  - Golden Rule: don't call `fit` on your validation data.\n",
    "  - But in my cross-validation splits, the validation portion will contain some of the original training data, which was used to `fit` the preprocessor.\n",
    "  - Thus, the cross-validation training data was preprocessed \"knowing something\" about the cross-validation validation data.\n",
    "     - So the cross-validation validation data is not truly \"unseen data\".\n",
    "     - This is a \"leak\".\n",
    "  - This is _extremely confusing_ because we have the original train/validation and the cross-validation train/validation.\n",
    "  - Draw a picture here!\n",
    "- I accidentally violated the Golden Rule in hw4 exercise 4(c) doing this with `RandomizedSearchCV`.\n",
    "  - Let's take a look there for a minute.\n",
    "  - I'll update the solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so I'll only use the train split - which I also suggested in the hw4 solution. Does this solve the problem?\n",
    "  - Discuss for 2-3 minutes.\n",
    "  - Don't look ahead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.82163027, 0.94802539, 0.88206276, 0.91522733, 0.56098685,\n",
       "       0.84892397, 0.86539653, 0.89571204, 0.90336091, 0.9062442 ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(model, X_train_enc, y_train_log, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "\n",
    "- No! I still have folds where the train split was preprocessed based on data in the validation split.\n",
    "- Ok, so I'll fit the preprocessing on the combined \"trainvalid\" split - does that solve the problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor2 = ColumnTransformer([\n",
    "    ('numeric', numeric_transformer, numeric_features),\n",
    "    ('ordinal', ordinal_transformer, ordinal_features_reg),\n",
    "    ('categorical', categorical_transformer, categorical_features + ordinal_features_oth)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainvalid_enc2 = preprocessor2.fit_transform(df_trainvalid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91330975, 0.86542043, 0.84417442, 0.87904043, 0.90593103,\n",
       "       0.89385594, 0.86962887, 0.90348459, 0.5922736 , 0.91025991])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(model, X_trainvalid_enc2, y_trainvalid_log, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "\n",
    "- No! Same problem again!\n",
    "- What if we use a `Pipeline`?\n",
    "  - So far we just used a `Pipeline` for the preprocessing.\n",
    "  - This time we'll combine **the preprocessing and the model** with a `Pipeline` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line fits the transformer **and** the model; there was no need to call `preprocessor.fit()` before this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(df_train, y_train_log);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how I passed in `df_train`, not `df_train_enc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.97330353, 11.69417912, 11.9050535 , ..., 12.20638141,\n",
       "       11.99713426, 11.81988184])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8963159928298279"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(df_train, y_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8913218700485418"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(df_valid, y_valid_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this `Pipeline` on the combined train and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91329227, 0.86442022, 0.84424495, 0.87895509, 0.90585488,\n",
       "       0.89379337, 0.86963715, 0.90347339, 0.58863687, 0.91024683])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(pipeline, df_trainvalid, y_trainvalid_log, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Does this solve the problem?\n",
    "  - Discuss for 2-3 minutes.\n",
    "  - Don't look ahead!\n",
    "  \n",
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Why does this work?\n",
    "\n",
    "- Because `cross_val_score` calls `fit` for each fold.\n",
    "- And this includes fitting the preprocessor.\n",
    "- Thus, there is actually no difference between `df_train` and `df_valid` - nothing has actually been done to them yet!\n",
    "  - `df_trainvalid` is just the part that's not the test set, that's all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(optional note) Yet another idea could be to do the cross-validatin using only the validation split. I believe this does not technically violate the Golden Rule, but it's very wasteful in terms of data and is also not really representative of how your model will eventually be trained. The `Pipeline` approach is much better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pipelines and hyperparameter optimization (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The same problems arise when doing hyperparameter optimization, simply because these methods do cross-validation.\n",
    "- We can avoid them with a `Pipeline` in the same way.\n",
    "- I'll optimize `alpha`, so I'll create a new pipeline where `alpha` is not specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('preprocessor', preprocessor),\n",
    "                      ('model', Ridge())])\n",
    "\n",
    "param_grid = {\n",
    "    'preprocessor__numeric__imputer__strategy': ['mean', 'median'],\n",
    "    'model__alpha': [1.0, 10, 100],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above: we have a nesting of transformers. \n",
    "- We can access the parameters of the \"inner\" objects by using `__` to go \"deeper\":\n",
    "  - `model__alpha`: \"the `alpha` of the model (of the pipeline)\"\n",
    "  - `preprocessor__numeric__imputer__strategy`: \"the strategy of the imputer of the numeric transformer of the preprocessor (of the pipeline)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipeline, param_grid, cv=10)\n",
    "grid_search.fit(df_trainvalid, y_trainvalid_log);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__alpha': 10, 'preprocessor__numeric__imputer__strategy': 'median'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is particularly useful when there are serious hyperparameters in the preprocessing pipeline, e.g. if you're using `CountVectorizer`."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
