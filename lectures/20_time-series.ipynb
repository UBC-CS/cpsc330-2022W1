{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Lecture 20: Time series\n",
    "\n",
    "UBC 2022-23\n",
    "\n",
    "Instructor: Varada Kolhatkar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Imports, announcements, LO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import (\n",
    "    TimeSeriesSplit,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning objectives\n",
    "\n",
    "- Recognize when it is appropriate to use time series. \n",
    "- Explain the pitfalls of train/test splitting with time series data.\n",
    "- Appropriately split time series data, both train/test split and cross-validation.\n",
    "- Perform time series feature engineering:\n",
    "  - Encode time as various features in a tabular dataset \n",
    "  - Create lag-based features\n",
    "- Explain how can you forecast multiple time steps into the future.\n",
    "- Explain the challenges of time series data with unequally spaced time points.\n",
    "- At a high level, explain the concept of trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "- **Time series** is a collection of data points indexed in time order. \n",
    "- Time series is everywhere:\n",
    "    - Physical sciences (e.g., weather forecasting)  \n",
    "    - Economics, finance (e.g., stocks, market trends)\n",
    "    - Engineering (e.g., energy consumption)\n",
    "    - Social sciences \n",
    "    - Sports analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's start with a simple example from [Introduction to Machine Learning with Python  book](https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/). \n",
    "\n",
    "In New York city there is a network of bike rental stations with a subscription system. The stations are all around the city. The anonymized data is available [here](https://ride.citibikenyc.com/system-data).\n",
    "\n",
    "We will focus on the task is predicting how many people will rent a bicycle from a particular station for a given time and day. We might be interested in knowing this so that we know whether there will be any bikes left at the station for a particular day and time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import mglearn\n",
    "\n",
    "citibike = mglearn.datasets.load_citibike()\n",
    "citibike.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The only feature we have is the date time feature. \n",
    "    - Example: 2015-08-01 00:00:00\n",
    "- The target is the number of rentals in the next 3 hours. \n",
    "    - Example: 3 rentals between 2015-08-01 00:00:00 and 2015-08-01 03:00:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the time duration of our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citibike.index.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citibike.index.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have data for August 2015. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "xticks = pd.date_range(start=citibike.index.min(), end=citibike.index.max(), freq=\"D\")\n",
    "plt.xticks(xticks, xticks.strftime(\"%a %m-%d\"), rotation=90, ha=\"left\")\n",
    "plt.plot(citibike, linewidth=1)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Rentals\");\n",
    "plt.title(\"Number of bike rentals over time for a selected bike station\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We see the day and night pattern\n",
    "- We see the weekend and weekday pattern "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Questions you might want to answer: How many people are likely to rent a bike at this station in the next three hours given everything we know about rentals in the past? \n",
    "- We want to learn from the past and predict the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use our usual machine learning methodology for this problem? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train/test split for temporal data\n",
    "\n",
    "- What will happen if we split this data the usual way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(citibike, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.index.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- So, we are training on data that came after our test data!\n",
    "- If we want to forecast, **we aren't allowed to know what happened in the future**!\n",
    "- There may be cases where this is OK, e.g. if you aren't trying to forecast and just want to understand your data (maybe you're not even splitting).\n",
    "- But, for our purposes, we want to avoid this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "train_df_sort = train_df.sort_index()\n",
    "test_df_sort = test_df.sort_index()\n",
    "\n",
    "plt.plot(train_df_sort, \"b\", label=\"train\")\n",
    "plt.plot(test_df_sort, \"r\", label=\"test\")\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll split the data as follows:\n",
    "\n",
    "- We have total 248 data points. \n",
    "- We'll use the fist 184 data points corresponding to the first 23 days as training data - And the remaining 64 data points corresponding to the remaining 8 days as test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "citibike.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 184\n",
    "train_df = citibike[:184]\n",
    "test_df = citibike[184:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "train_df_sort = train_df.sort_index()\n",
    "test_df_sort = test_df.sort_index()\n",
    "\n",
    "plt.plot(train_df_sort, \"b\", label=\"train\")\n",
    "plt.plot(test_df_sort, \"r\", label=\"test\")\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This split is looking reasonable now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training models \n",
    "\n",
    "- In this toy data, we just have a single feature: the date time feature. \n",
    "- We need to encode this feature if we want to build machine learning models. \n",
    "- A common way that dates are stored on computers is using POSIX time, which is the number of seconds since January 1970 00:00:00 (this is beginning of Unix time). \n",
    "- Let's start with encoding this feature as a single integer representing this POSIX time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = (\n",
    "    citibike.index.astype(\"int64\").values.reshape(-1, 1) // 10 ** 9\n",
    ")  # convert to POSIX time by dividing by 10**9\n",
    "y = citibike.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df.values\n",
    "y_test = test_df.values\n",
    "# convert to POSIX time by dividing by 10**9\n",
    "X_train = train_df.index.astype(\"int64\").values.reshape(-1, 1) // 10 ** 9\n",
    "X_test = test_df.index.astype(\"int64\").values.reshape(-1, 1) // 10 ** 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Our prediction task is a regression task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's try random forest regression with just this feature. \n",
    "- We'll be trying out many different features. So we'll be using the function below which\n",
    "    - Splits the data \n",
    "    - Trains the given regressor model on the training data\n",
    "    - Shows train and test scores\n",
    "    - Plots the predictions on the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Code credit: Adapted from \n",
    "# https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/\n",
    "\n",
    "def eval_on_features(features, target, regressor, n_train = 184, sales_data=False, ylabel='Rentals'):\n",
    "    # split the given features into a training and a test set\n",
    "    X_train, X_test = features[:n_train], features[n_train:]\n",
    "    # also split the target array\n",
    "    y_train, y_test = target[:n_train], target[n_train:]\n",
    "    regressor.fit(X_train, y_train)\n",
    "    print(\"Train-set R^2: {:.2f}\".format(regressor.score(X_train, y_train)))\n",
    "    print(\"Test-set R^2: {:.2f}\".format(regressor.score(X_test, y_test)))\n",
    "    y_pred = regressor.predict(X_test)\n",
    "    y_pred_train = regressor.predict(X_train)\n",
    "    plt.figure(figsize=(10, 3))\n",
    "\n",
    "    if not sales_data: \n",
    "        plt.xticks(range(0, len(X), 8), xticks.strftime(\"%a %m-%d\"), rotation=90, ha=\"left\")\n",
    "        \n",
    "    plt.plot(range(n_train), y_train, label=\"train\")\n",
    "    plt.plot(range(n_train, len(y_test) + n_train), y_test, \"-\", label=\"test\")\n",
    "    plt.plot(range(n_train), y_pred_train, \"--\", label=\"prediction train\")\n",
    "\n",
    "    plt.plot(\n",
    "        range(n_train, len(y_test) + n_train), y_pred, \"--\", label=\"prediction test\"\n",
    "    )\n",
    "    plt.legend(loc=(1.01, 0))\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(ylabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try random forest regressor with our posix time feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "eval_on_features(X, y, regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The predictions on the training score is pretty good \n",
    "- But for the test data, a constant line is predicted ...\n",
    "- What's going on? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The model is based on only one feature: POSIX time feature. \n",
    "- And the value of the POSIX time feature is outside the range of the feature values in the training set. \n",
    "- Tree-based models cannot _extrapolate_ to feature ranges outside the training data. \n",
    "- The model predicted the target value of the closest point in the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we come up with better features? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature engineering for date/time columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that our index is of this special type: [`DateTimeIndex`](https://pandas.pydata.org/docs/reference/api/pandas.DatetimeIndex.html). We can extract all kinds of interesting information from it.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citibike.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "citibike.index.month_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "citibike.index.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citibike.index.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citibike.index.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We noted before that the time of the day and day of the week seem quite important. \n",
    "- Let's add these two features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's first add the time of the day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hour = citibike.index.hour.values.reshape(-1, 1)\n",
    "X_hour[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "eval_on_features(X_hour, y, regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores are better when we add time of the day feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's add day of the week along with time of the day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "X_hour_week = np.hstack(\n",
    "    [\n",
    "        citibike.index.dayofweek.values.reshape(-1, 1),\n",
    "        citibike.index.hour.values.reshape(-1, 1),\n",
    "    ]\n",
    ")\n",
    "eval_on_features(X_hour_week, y, regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are much better. The time of the day and day of the week features are clearly helping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Do we need a complex model such as a random forest? \n",
    "- Let's try `Ridge` with these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "lr = Ridge()\n",
    "eval_on_features(X_hour_week, y, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Ridge is performing poorly on the training as well as test data.\n",
    "- It's not able to capture the periodic pattern.\n",
    "- The reason is that we have encoded time of day using integers. \n",
    "- A linear function can only learn a linear function of the time of day. \n",
    "- What if we encode this feature as a categorical variable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "X_hour_week_onehot = enc.fit_transform(X_hour_week).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hour_week_onehot\n",
    "X_hour_week_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour = [\"%02d:00\" % i for i in range(0, 24, 3)]\n",
    "day = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "features = day + hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_hour_week_onehot, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_on_features(X_hour_week_onehot, y, Ridge())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- What if we add interaction features (e.g., something like Day == 5 and hour == 9:00)\n",
    "- We can do it using `sklearn`'s `PolynomialFeatures` transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_transformer = PolynomialFeatures(\n",
    "    interaction_only=True, include_bias=False\n",
    ")\n",
    "X_hour_week_onehot_poly = poly_transformer.fit_transform(X_hour_week_onehot)\n",
    "lr = Ridge()\n",
    "eval_on_features(X_hour_week_onehot_poly, y, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hour_week_onehot_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "hour = [\"%02d:00\" % i for i in range(0, 24, 3)]\n",
    "day = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "features = day + hour\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's examine the coefficients learned by `Ridge`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "features_poly = poly_transformer.get_feature_names_out(features)\n",
    "features_nonzero = np.array(features_poly)[lr.coef_ != 0]\n",
    "coef_nonzero = lr.coef_[lr.coef_ != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(coef_nonzero, index=features_nonzero, columns=[\"Coefficient\"]).sort_values(\n",
    "    \"Coefficient\", ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The coefficients make sense!\n",
    "- If it's Saturday 09:00 or Wednesday 06:00, the model is likely to predict bigger number for rentals. \n",
    "- If it's Midnight or 03:00 or Sunday 06:00, the model is likely to predict smaller number for rentals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lag-based features\n",
    "\n",
    "- So far we engineered some features and managed to get reasonable results. \n",
    "- In time series data there is temporal dependence; observations close in time tend to be correlated. \n",
    "- Currently we're using current time to predict the number of bike rentals at that time. \n",
    "- But, what if the number of bike rentals is also related to bike rentals three hours ago or 6 hours ago and so on? \n",
    "  - Such features are called _lagged_ features.  \n",
    "  \n",
    "_Note: In time series analysis, you would look at something called an [autocorrelation function](https://en.wikipedia.org/wiki/Autocorrelation) (ACF), but we won't go into that here._ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's extract lag features. We can \"lag\" (or \"shift\") a time series in Pandas with the `.shift()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citibike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rentals_df = pd.DataFrame(citibike)\n",
    "rentals_df = rentals_df.rename(columns={\"one\":\"n_rentals\"})\n",
    "rentals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_df(df, lag, cols):\n",
    "    return df.assign(\n",
    "        **{f\"{col}-{n}\": df[col].shift(n) for n in range(1, lag + 1) for col in cols}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rentals_lag5 = create_lag_df(rentals_df, 5, ['n_rentals'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rentals_lag5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note the NaN pattern. \n",
    "- We can either drop the first 5 rows or apply imputation. \n",
    "- Let's try these features with Ridge model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lag_features = rentals_lag5.drop(columns = ['n_rentals']).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer()\n",
    "X_lag_features_imp = imp.fit_transform(X_lag_features)\n",
    "lr = Ridge()\n",
    "eval_on_features(X_lag_features_imp, y, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not great. But let's examine the coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lag = rentals_lag5.drop(columns=['n_rentals']).columns.tolist()\n",
    "features_nonzero = np.array(features_lag)[lr.coef_ != 0]\n",
    "coef_nonzero = lr.coef_[lr.coef_ != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(coef_nonzero, index=features_nonzero, columns=[\"Coefficient\"]).sort_values(\n",
    "    \"Coefficient\", ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about `RandomForestRegressor` model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer()\n",
    "X_lag_features_imp = imp.fit_transform(X_lag_features)\n",
    "rf = RandomForestRegressor()\n",
    "eval_on_features(X_lag_features_imp, y, rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not as good as the results with our preciously engineered features. How about combining lag-based features and the previously extracted features? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hour_week_onehot_poly_lag = np.hstack([X_hour_week_onehot_poly, X_lag_features_imp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "eval_on_features(X_hour_week_onehot_poly_lag, y, rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some improvement but we are getting better results without the lag features in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross-validation\n",
    "\n",
    "What about cross-validation? \n",
    "\n",
    "- We can't do regular cross-validation if we don't want to be predicting the past.\n",
    "- If you carry out regular cross-validation, you'll be predicting the past given future which is not a realistic scenario for the deployment data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_cross_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There is [`TimeSeriesSplit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) for time series data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from sklearn documentation\n",
    "X_toy = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y_toy = np.array([1, 2, 3, 4, 5, 6])\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "for train, test in tscv.split(X_toy):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try it out with Ridge on the bike rental data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lr = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(\n",
    "    lr, X_hour_week_onehot_poly, y, cv=TimeSeriesSplit(), return_train_score=True\n",
    ")\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forecasting further into the future\n",
    "\n",
    "Let's consider another time series dataset, [Retail Sales of Clothing and Clothing Accessory Stores dataset](https://fred.stlouisfed.org/series/MRTSSM448USN) which is made available by the Federal Reserve Bank of St. Louis.\n",
    "\n",
    "The dataset has dates and retail sales values corresponding to the dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_df = pd.read_csv(\"data/MRTSSM448USN.csv\", parse_dates=[\"DATE\"])\n",
    "retail_df.columns = [\"date\", \"sales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the min and max dates in order to split the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "retail_df[\"date\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_df[\"date\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm considering everything upto 01 January 2016 as training data and everything after that as test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_df_train = retail_df.query(\"date <= 20160101\")\n",
    "retail_df_test = retail_df.query(\"date >  20160101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_df_train.plot(x=\"date\", y=\"sales\", figsize=(15, 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can create a dataset using purely lag features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_df(df, lag, cols):\n",
    "    return df.assign(\n",
    "        **{f\"{col}-{n}\": df[col].shift(n) for n in range(1, lag + 1) for col in cols}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create lag features for the full dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_lag_5 = lag_df(retail_df, 5, [\"sales\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_train_5 = retail_lag_5.query(\"date <= 20160101\")\n",
    "retail_test_5 = retail_lag_5.query(\"date >  20160101\")\n",
    "retail_train_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Now, if we drop the \"date\" column we have a target (\"sales\") and 5 features (the previous 5 days of sales).\n",
    "- We need to impute/drop the missing values and then we can fit a model to this. I will just drop for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_train_5 = retail_train_5[5:].drop(columns=[\"date\"])\n",
    "retail_train_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "retail_train_5_X = retail_train_5.drop(columns=[\"sales\"])\n",
    "retail_train_5_y = retail_train_5[\"sales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_model = RandomForestRegressor(random_state=42)\n",
    "retail_model.fit(retail_train_5_X, retail_train_5_y);\n",
    "print(\"Train-set R^2: {:.2f}\".format(retail_model.score(retail_train_5_X, retail_train_5_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this, we can now predict the sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = retail_model.predict(retail_test_5.drop(columns=[\"date\", \"sales\"]))\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_test_5_preds = retail_test_5.assign(predicted_sales=preds)\n",
    "retail_test_5_preds.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ok, that is fine, but what if we want to predict 5 months in the future? \n",
    "- Well, we would not have access to our features!! We don't yet know the previous months sales, or two months prior! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are a few approaches which could be employed:\n",
    "\n",
    "1. Train a separate model for each number of months. E.g. one model that predicts sales for next month, another model that predicts sales in two months, etc. We can build these datasets.\n",
    "2. Use a multi-output model that jointly predicts RainTomorrow, RainIn2Days, etc. However, multi-output models are outside the scope of CPSC 330. \n",
    "3. Use one model and sequentially predict using a `for` loop. However, this requires predicting _all_ features into a model so may not be that useful here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we decide to use approach 3, we would predict these values and then pretend they are true! For example, given that it's November 2022\n",
    "\n",
    "1. Predict December 2022 sales\n",
    "2. Then, to predict for January 2023, we need to know December 2022 sales. Use our _prediction_ for December 2022 as the truth.\n",
    "3. Then, to predict for February 2023, we need to know December 2022 and January 2023 sales. Use our predictions.\n",
    "4. Etc etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trends \n",
    "\n",
    "- There are some important concepts in time series that rely on having a continuous target (like we do in the retail sales example above).\n",
    "- Part of that is the idea of seasonality and trends.\n",
    "- These are mostly taken care of by our feature engineering of the data variable, but there's something important left to discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "retail_df_train.plot(x=\"date\", y=\"sales\", figsize=(15, 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It looks like there's a **trend** here - the sales are going up over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's say we encoded the date as a feature in days like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_train_5_date = retail_lag_5.query(\"date <= 20160101\")\n",
    "first_day_retail = retail_train_5_date[\"date\"].min()\n",
    "\n",
    "retail_train_5_date = retail_train_5_date.assign(\n",
    "    Days_since=retail_train_5_date[\"date\"].apply(lambda x: (x - first_day_retail).days)\n",
    ")\n",
    "retail_train_5_date.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, let's say we use all these features (the lagged version of the target and also `Days_since`.\n",
    "- If we use **linear regression** we'll learn a coefficient for `Days_since`. \n",
    "  - If that coefficient is positive, it predicts unlimited growth forever. That may not be what you want? It depends.\n",
    "- If we use a **random forest**, we'll just be doing splits from the training set, e.g. \"if `Days_since` > 9100 then do this\".\n",
    "  - There will be no splits for later time points because there is no training data there.\n",
    "  - Thus tree-based models cannot model trends.\n",
    "  - This is really important to know!!\n",
    "- Often, we model the trend separately and use the random forest to model a de-trended time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A more complicated dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Rain in Australia](https://www.kaggle.com/jsphyg/weather-dataset-rattle-package) dataset. Predicting whether or not it will rain tomorrow based on today's measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df = pd.read_csv(\"data/weatherAUS.csv\")\n",
    "rain_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Goals\n",
    "\n",
    "- Can the date/time features help us predict the target value?\n",
    "- Can we **forecast** into the future? Can we predict whether it's going to rain tomorrow? \n",
    "- The target variable is `RainTomorrow`. The target is categorical and not continuous in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rain_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rain_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- A number of missing values. \n",
    "- Some target values are also missing. I'm dropping these rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df = rain_df[rain_df[\"RainTomorrow\"].notna()]\n",
    "rain_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Parsing datetimes \n",
    "\n",
    "- In general, datetimes are a huge pain! \n",
    "    - Think of all the formats: MM-DD-YY, DD-MM-YY, YY-MM-DD, MM/DD/YY, DD/MM/YY, DD/MM/YYYY, 20:45, 8:45am, 8:45 PM, 8:45a, 08:00, 8:10:20, .......\n",
    "  - Time zones.\n",
    "  - Daylight savings...\n",
    "- Thankfully, pandas does a pretty good job here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain = pd.to_datetime(rain_df[\"Date\"])\n",
    "dates_rain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "They are all the same format, so we can also compare dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain[1] - dates_rain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain[1] > dates_rain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dates_rain[1] - dates_rain[0]).total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also easily extract information from the date columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain[1].month_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain[1].day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain[1].is_year_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain[1].is_leap_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Above pandas identified the date column automatically. You can tell pandas to parse the dates when reading in the CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df = pd.read_csv(\"data/weatherAUS.csv\", parse_dates=[\"Date\"])\n",
    "rain_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df = rain_df[rain_df[\"RainTomorrow\"].notna()]\n",
    "rain_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df[\"Date\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train/test splits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remember that we should not be calling the usual `train_test_split` with shuffling because \n",
    "- If we want to forecast, we aren't allowed to know what happened in the future!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df[\"Date\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df[\"Date\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It looks like we have 10 years of data.\n",
    "- Let's use the last 2 years for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df = rain_df.query(\"Date <= 20150630\")\n",
    "test_df = rain_df.query(\"Date >  20150630\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "len(test_df) / (len(train_df) + len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As we can see, we're still using about 25% of our data as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_sort = train_df.query(\"Location == 'Sydney'\").sort_values(by=\"Date\")\n",
    "test_df_sort = test_df.query(\"Location == 'Sydney'\").sort_values(by=\"Date\")\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(train_df_sort[\"Date\"], train_df_sort[\"Rainfall\"], \"b\", label=\"train\")\n",
    "plt.plot(test_df_sort[\"Date\"], test_df_sort[\"Rainfall\"], \"r\", label=\"test\")\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Rainfall (mm)\")\n",
    "plt.title(\"Train/test rainfall in Sydney\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're learning relationships from the blue part; predicting only using features in the red part from the day before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's define a preprocessor with a column transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have missing data. \n",
    "- We have categorical features and numeric features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's define feature types. \n",
    "- Let's start with dropping the date column and treating it as a usual supervised machine learning problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    \"MinTemp\",\n",
    "    \"MaxTemp\",\n",
    "    \"Rainfall\",\n",
    "    \"Evaporation\",\n",
    "    \"Sunshine\",\n",
    "    \"WindGustSpeed\",\n",
    "    \"WindSpeed9am\",\n",
    "    \"WindSpeed3pm\",\n",
    "    \"Humidity9am\",\n",
    "    \"Humidity3pm\",\n",
    "    \"Pressure9am\",\n",
    "    \"Pressure3pm\",\n",
    "    \"Cloud9am\",\n",
    "    \"Cloud3pm\",\n",
    "    \"Temp9am\",\n",
    "    \"Temp3pm\",\n",
    "]\n",
    "categorical_features = [\n",
    "    \"Location\",\n",
    "    \"WindGustDir\",\n",
    "    \"WindDir9am\",\n",
    "    \"WindDir3pm\",\n",
    "    \"RainToday\",\n",
    "]\n",
    "drop_features = [\"Date\"]\n",
    "target = [\"RainTomorrow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features,\n",
    "    categorical_features,\n",
    "    drop_features,\n",
    "    target\n",
    "):\n",
    "\n",
    "    all_features = set(numeric_features + categorical_features + drop_features + target)\n",
    "    if set(train_df.columns) != all_features:\n",
    "        print(\"Missing columns\", set(train_df.columns) - all_features)\n",
    "        print(\"Extra columns\", all_features - set(train_df.columns))\n",
    "        raise Exception(\"Columns do not match\")\n",
    "\n",
    "    numeric_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"), StandardScaler()\n",
    "    )\n",
    "    categorical_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "        OneHotEncoder(handle_unknown=\"ignore\", sparse=False),\n",
    "    )\n",
    "\n",
    "    preprocessor = make_column_transformer(\n",
    "        (numeric_transformer, numeric_features),\n",
    "        (categorical_transformer, categorical_features),\n",
    "        (\"drop\", drop_features),\n",
    "    )\n",
    "    preprocessor.fit(train_df)\n",
    "    ohe_feature_names = (\n",
    "        preprocessor.named_transformers_[\"pipeline-2\"]\n",
    "        .named_steps[\"onehotencoder\"]\n",
    "        .get_feature_names_out(categorical_features)\n",
    "        .tolist()\n",
    "    )\n",
    "    new_columns = numeric_features + ohe_feature_names\n",
    "\n",
    "    X_train_enc = pd.DataFrame(\n",
    "        preprocessor.transform(train_df), index=train_df.index, columns=new_columns\n",
    "    )\n",
    "    X_test_enc = pd.DataFrame(\n",
    "        preprocessor.transform(test_df), index=test_df.index, columns=new_columns\n",
    "    )\n",
    "\n",
    "    y_train = train_df[\"RainTomorrow\"]\n",
    "    y_test = test_df[\"RainTomorrow\"]\n",
    "\n",
    "    return X_train_enc, y_train, X_test_enc, y_test, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features,\n",
    "    categorical_features,\n",
    "    drop_features, target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `DummyClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = DummyClassifier()\n",
    "dc.fit(train_df, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.score(train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.score(test_df, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The function below trains a logistic regression model on the train set, reports train and test scores, and returns learned coefficients as a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def score_lr_print_coeff(preprocessor, train_df, y_train, test_df, y_test, X_train_enc):\n",
    "    lr_pipe = make_pipeline(preprocessor, LogisticRegression(max_iter=1000))\n",
    "    lr_pipe.fit(train_df, y_train)\n",
    "    print(\"Train score: {:.2f}\".format(lr_pipe.score(train_df, y_train)))\n",
    "    print(\"Test score: {:.2f}\".format(lr_pipe.score(test_df, y_test)))\n",
    "    lr_coef = pd.DataFrame(\n",
    "        data=lr_pipe.named_steps[\"logisticregression\"].coef_.flatten(),\n",
    "        index=X_train_enc.columns,\n",
    "        columns=[\"Coef\"],\n",
    "    )\n",
    "    return lr_coef.sort_values(by=\"Coef\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "score_lr_print_coeff(preprocessor, train_df, y_train, test_df, y_test, X_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross-validation\n",
    "\n",
    "- We can carry out cross-validation using [`TimeSeriesSplit`](https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split). \n",
    "- However, things are actually more complicated here because this dataset has **multiple time series**, one per location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sort_values(by=[\"Date\", \"Location\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.sort_values(by=[\"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- It seems the dataframe is sorted by location, and then time. \n",
    "- Our approach today will be to ignore the fact that we have multiple time series and just OHE the location\n",
    "- We'll have multiple measurements for a given timestamp, and that's OK.\n",
    "- But, `TimeSeriesSplit` expects the dataframe to be sorted by date so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df_ordered = train_df.sort_values(by=[\"Date\"])\n",
    "y_train_ordered = train_df_ordered[\"RainTomorrow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lr_pipe = make_pipeline(preprocessor, LogisticRegression(max_iter=1000))\n",
    "cross_val_score(lr_pipe, train_df_ordered, y_train_ordered, cv=TimeSeriesSplit()).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Encoding date/time as feature(s)\n",
    "\n",
    "- Can we use the `Date` to help us predict the target?\n",
    "- Probably! E.g. different amounts of rain in different seasons.\n",
    "- This is feature engineering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Encoding time as an number\n",
    "\n",
    "- Idea 1: create a column of \"days since Nov 1, 2007\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = rain_df.query(\"Date <= 20150630\")\n",
    "test_df = rain_df.query(\"Date >  20150630\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_day = train_df[\"Date\"].min()\n",
    "\n",
    "train_df = train_df.assign(\n",
    "    Days_since=train_df[\"Date\"].apply(lambda x: (x - first_day).days)\n",
    ")\n",
    "test_df = test_df.assign(\n",
    "    Days_since=test_df[\"Date\"].apply(lambda x: (x - first_day).days)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sort_values(by=\"Date\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features + [\"Days_since\"],\n",
    "    categorical_features,\n",
    "    drop_features,\n",
    "    target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_lr_print_coeff(preprocessor, train_df, y_train, test_df, y_test, X_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Not much improvement in the scores\n",
    "- Can you think of other ways to generate features from the `Date` column? \n",
    "- What about the month - that seems relevant. How should we encode the month? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Encode it as a categorical variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding of the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = rain_df.query(\"Date <= 20150630\")\n",
    "test_df = rain_df.query(\"Date >  20150630\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.assign(\n",
    "    Month=train_df[\"Date\"].apply(lambda x: x.month_name())\n",
    ")  # x.month_name() to get the actual string\n",
    "test_df = test_df.assign(Month=test_df[\"Date\"].apply(lambda x: x.month_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[[\"Date\", \"Month\"]].sort_values(by=\"Month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    train_df, test_df, \n",
    "    numeric_features, \n",
    "    categorical_features + [\"Month\"], \n",
    "    drop_features,\n",
    "    target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_lr_print_coeff(preprocessor, train_df, y_train, test_df, y_test, X_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No change in the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One-hot encoding seasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How about just summer/winter as a feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(month):\n",
    "    # remember this is Australia\n",
    "    WINTER_MONTHS = [\"June\", \"July\", \"August\"] \n",
    "    AUTUMN_MONTHS = [\"March\", \"April\", \"May\"]\n",
    "    SUMMER_MONTHS = [\"December\", \"January\", \"February\"]\n",
    "    SPRING_MONTHS = [\"September\", \"October\", \"November\"]\n",
    "    if month in WINTER_MONTHS:\n",
    "        return \"Winter\"\n",
    "    elif month in AUTUMN_MONTHS:\n",
    "        return \"Autumn\"\n",
    "    elif month in SUMMER_MONTHS:\n",
    "        return \"Summer\"\n",
    "    else:\n",
    "        return \"Fall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.assign(Season=train_df[\"Month\"].apply(get_season))\n",
    "test_df = test_df.assign(Season=test_df[\"Month\"].apply(get_season))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features,\n",
    "    categorical_features + [\"Season\"],\n",
    "    drop_features + [\"Month\"],\n",
    "    target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df = score_lr_print_coeff(\n",
    "    preprocessor, train_df, y_train, test_df, y_test, X_train_enc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "coeff_df.loc[[\"Season_Fall\", \"Season_Summer\", \"Season_Winter\", \"Season_Autumn\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No improvements in the scores but the coefficients make some sense,\n",
    "- A negative coefficient for summer and a positive coefficients for winter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.plot(x=\"Date\", y=\"Rainfall\")\n",
    "plt.ylabel(\"Rainfall (mm)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lag-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df = rain_df.query(\"Date <= 20150630\")\n",
    "test_df = rain_df.query(\"Date >  20150630\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It looks like the dataframe is already sorted by Location and then by date for each Location.\n",
    "- We could have done this ourselves with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.sort_values(by=[\"Location\", \"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But make sure to also sort the targets (i.e. do this before preprocessing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can \"lag\" (or \"shift\") a time series in Pandas with the .shift() method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.assign(Rainfall_lag1=train_df[\"Rainfall\"].shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[[\"Date\", \"Location\", \"Rainfall\", \"Rainfall_lag1\"]][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- But we have multiple time series here and we need to be more careful with this. \n",
    "- When we switch from one location to another we do not want to take the value from the previous location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def create_lag_feature(df, orig_feature, lag):\n",
    "    \"\"\"Creates a new df with a new feature that's a lagged version of the original, where lag is an int.\"\"\"\n",
    "    # note: pandas .shift() kind of does this for you already, but oh well I already wrote this code\n",
    "\n",
    "    new_df = df.copy()\n",
    "    new_feature_name = \"%s_lag%d\" % (orig_feature, lag)\n",
    "    new_df[new_feature_name] = np.nan\n",
    "    for location, df_location in new_df.groupby(\n",
    "        \"Location\"\n",
    "    ):  # Each location is its own time series\n",
    "        new_df.loc[df_location.index[lag:], new_feature_name] = df_location.iloc[:-lag][\n",
    "            orig_feature\n",
    "        ].values\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df = create_lag_feature(train_df, \"Rainfall\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[[\"Date\", \"Location\", \"Rainfall\", \"Rainfall_lag1\"]][2285:2295]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it looks good! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Question: is it OK to do this to the test set? Discuss.\n",
    "- It's fine if you would have this information available in deployment.\n",
    "- If we're just forecasting the next day, we should.\n",
    "- Let's include it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rain_df_modified = create_lag_feature(rain_df, \"Rainfall\", 1)\n",
    "train_df = rain_df_modified.query(\"Date <= 20150630\")\n",
    "test_df = rain_df_modified.query(\"Date >  20150630\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features + [\"Rainfall_lag1\"],\n",
    "    categorical_features,\n",
    "    drop_features,\n",
    "    target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_coef = score_lr_print_coeff(\n",
    "    preprocessor, train_df, y_train, test_df, y_test, X_train_enc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_coef.loc[[\"Rainfall\", \"Rainfall_lag1\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rainfall from today has a positive coefficient. \n",
    "- Rainfall from yesterday has a positive but a smaller coefficient. \n",
    "- If we didn't have rainfall from today feature, rainfall from yesterday feature would have received a bigger coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We could also create a lagged version of the target.\n",
    "- In fact, this dataset already has that built in! `RainToday` is the lagged version of the target `RainTomorrow`.\n",
    "- We could also create lagged version of other features, or more lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rain_df_modified = create_lag_feature(rain_df, \"Rainfall\", 1)\n",
    "rain_df_modified = create_lag_feature(rain_df_modified, \"Rainfall\", 2)\n",
    "rain_df_modified = create_lag_feature(rain_df_modified, \"Rainfall\", 3)\n",
    "rain_df_modified = create_lag_feature(rain_df_modified, \"Humidity3pm\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df_modified[\n",
    "    [\n",
    "        \"Date\",\n",
    "        \"Location\",\n",
    "        \"Rainfall\",\n",
    "        \"Rainfall_lag1\",\n",
    "        \"Rainfall_lag2\",\n",
    "        \"Rainfall_lag3\",\n",
    "        \"Humidity3pm\",\n",
    "        \"Humidity3pm_lag1\",\n",
    "    ]\n",
    "].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the pattern of `NaN` values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df = rain_df_modified.query(\"Date <= 20150630\")\n",
    "test_df = rain_df_modified.query(\"Date >  20150630\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features\n",
    "    + [\"Rainfall_lag1\", \"Rainfall_lag2\", \"Rainfall_lag3\", \"Humidity3pm_lag1\"],\n",
    "    categorical_features,\n",
    "    drop_features,\n",
    "    target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_coef = score_lr_print_coeff(\n",
    "    preprocessor, train_df, y_train, test_df, y_test, X_train_enc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lr_coef.loc[\n",
    "    [\n",
    "        \"Rainfall\",\n",
    "        \"Rainfall_lag1\",\n",
    "        \"Rainfall_lag2\",\n",
    "        \"Rainfall_lag3\",\n",
    "        \"Humidity3pm\",\n",
    "        \"Humidity3pm_lag1\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note the pattern in the magnitude of the coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final remarks \n",
    "\n",
    "What did we not cover?\n",
    "\n",
    "- A huge amount!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional time series approaches\n",
    "\n",
    "- Time series analysis is a huge field of its own (notice a pattern here?)\n",
    "- Traditional approaches include the [ARIMA model](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average) and its various components/extensions.\n",
    "- In Python, the [statsmodels](https://www.statsmodels.org/) package is the place to go for this sort of thing.\n",
    "  - For example, [statsmodels.tsa.arima_model.ARIMA](https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima_model.ARIMA.html).\n",
    "- These approaches can forecast, but they are also very good for understanding the temporal relationships in your data.\n",
    "- We took different route in this course, and stick to our supervised learning tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning \n",
    "\n",
    "- Recently, deep learning has been very successful too.\n",
    "- In particular, [recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) (RNNs).\n",
    "  - These are not covered in CPSC 340, but I believe they are in 540 (soon to be renamed 440).\n",
    "  - [LSTMs](https://en.wikipedia.org/wiki/Long_short-term_memory) especially have shown a lot of promise in this type of task.\n",
    "  - [Here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) is a blog post about LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of problems involving time series\n",
    "\n",
    "- A single label associated with an entire time series. \n",
    "  - We had that with images earlier on, you could have the same for a time series.\n",
    "  - E.g., for fraud detection, labelling each transaction as fraud/normal vs. labelling a person as bad/good based on their entire history.\n",
    "  - There are various approaches that can be used for this type of problem, including CNNs (Lecture 19), LSTMs, and non deep learning methods.\n",
    "- Inference problems\n",
    "  - What are the patterns in this time series?\n",
    "  - How many lags are associated with the current value?\n",
    "  - Etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unequally spaced time points\n",
    "\n",
    "- We assumed we have a measurement each day.\n",
    "- For example, when creating lag features we used consecutive rows in the DataFrame.\n",
    "- But, in fact some days were missing in this dataset.\n",
    "- More generally, what if the measurements are at arbitrary times, not equally spaced?\n",
    "  - Some of our approaches would still work, like encoding the month.\n",
    "  - Some of our approaches would not make sense, like the lags.\n",
    "  - Perhaps the measurements could be binned into equally spaced bins, or something.\n",
    "  - This is more of a hassle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other software package\n",
    "\n",
    "- A good one to know about is [Prophet](https://facebook.github.io/prophet/docs/quick_start.html).\n",
    "- sktime\n",
    "    - Time series classification\n",
    "    - forecasting\n",
    "- tslearn \n",
    "    - classification\n",
    "    - regression\n",
    "    - clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "- Often, a useful approach is to just _engineer your own features_.\n",
    "  - E.g., max expenditure, min expenditure, max-min, avg time gap between transactions, variance of time gap between transactions, etc etc.\n",
    "  - We could do that here as well, or in any problem.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
